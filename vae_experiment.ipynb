{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "651fca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset import QuickDrawDataset\n",
    "from utils import svg_stroke5\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0c0017c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "def normalize_strokes(batch, scale=64.0):\n",
    "    batch = batch.clone().float()\n",
    "    # Normalize delta x, delta y\n",
    "    batch[..., 0:2] = batch[..., 0:2] / scale\n",
    "    # Leave pen_state as is (0, 1)\n",
    "    return batch\n",
    "\n",
    "def denormalize_strokes(batch, scale=64.0):\n",
    "    batch = batch.clone()\n",
    "    batch[..., 0:2] = batch[..., 0:2] * scale\n",
    "    return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d0dd2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading QuickDraw files: 100%|██████████| 1/1 [00:03<00:00,  3.18s/it]\n"
     ]
    }
   ],
   "source": [
    "labels = [\"cat\"]\n",
    "\n",
    "def to_tensor_svg_stroke5(svg_content):\n",
    "    return svg_stroke5(svg_content, bins=64).to(dtype=torch.int8)\n",
    "\n",
    "training_data = QuickDrawDataset(\n",
    "    labels=labels,\n",
    "    base_transform=to_tensor_svg_stroke5,\n",
    "    cache_file=\"data/quickdraw_stoke5_cache.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f91dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 | Loss: 2.468064 | Recon: 2.463628 | KL: 0.434701 | KL_weight: 0.010529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25 | Loss: 2.023833 | Recon: 2.019531 | KL: 0.398206 | KL_weight: 0.011085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25 | Loss: 1.885995 | Recon: 1.881513 | KL: 0.394125 | KL_weight: 0.011671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25 | Loss: 1.833480 | Recon: 1.829131 | KL: 0.363343 | KL_weight: 0.012288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25 | Loss: 1.805812 | Recon: 1.801808 | KL: 0.317750 | KL_weight: 0.012937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25 | Loss: 1.781940 | Recon: 1.778208 | KL: 0.281305 | KL_weight: 0.013621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25 | Loss: 1.767214 | Recon: 1.763705 | KL: 0.251155 | KL_weight: 0.014341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 | Loss: 1.759960 | Recon: 1.756479 | KL: 0.236593 | KL_weight: 0.015098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25 | Loss: 1.732675 | Recon: 1.729110 | KL: 0.230039 | KL_weight: 0.015896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 | Loss: 1.650333 | Recon: 1.646708 | KL: 0.222254 | KL_weight: 0.016737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 | Loss: 1.503337 | Recon: 1.499573 | KL: 0.219133 | KL_weight: 0.017621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25 | Loss: 1.445793 | Recon: 1.441858 | KL: 0.217661 | KL_weight: 0.018552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25 | Loss: 1.375055 | Recon: 1.370991 | KL: 0.213426 | KL_weight: 0.019533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25 | Loss: 1.328887 | Recon: 1.324621 | KL: 0.212857 | KL_weight: 0.020565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25 | Loss: 1.253135 | Recon: 1.248722 | KL: 0.209120 | KL_weight: 0.021652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 | Loss: 1.073836 | Recon: 1.069205 | KL: 0.208423 | KL_weight: 0.022796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25 | Loss: 0.924833 | Recon: 0.920010 | KL: 0.206174 | KL_weight: 0.024001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25 | Loss: 0.840641 | Recon: 0.835584 | KL: 0.205339 | KL_weight: 0.025270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25 | Loss: 0.750787 | Recon: 0.745358 | KL: 0.209277 | KL_weight: 0.026605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25 | Loss: 0.757285 | Recon: 0.751748 | KL: 0.202793 | KL_weight: 0.028011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25 | Loss: 0.676159 | Recon: 0.670356 | KL: 0.201879 | KL_weight: 0.029492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22:  74%|███████▍  | 765/1030 [00:33<00:11, 22.81it/s]"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Hyperparameters\n",
    "# -------------------------\n",
    "def default_hparams():\n",
    "    return {\n",
    "        \"decay_rate\": 0.9999,\n",
    "        \"grad_clip\": 1.0,\n",
    "        # KL annealing\n",
    "        \"kl_weight\": 0.5,\n",
    "        \"kl_weight_start\": 0.01,\n",
    "        \"kl_decay_rate\": 0.99995,\n",
    "        \"kl_tolerance\": 0.2,\n",
    "        # architecture\n",
    "        \"z_size\": 128,\n",
    "        \"enc_rnn_size\": 256,\n",
    "        \"dec_rnn_size\": 512,\n",
    "        \"num_mixture\": 20,\n",
    "        # data / training\n",
    "        \"max_seq_len\": 200,\n",
    "        \"batch_size\": 100,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"epochs\": 25,\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Model\n",
    "# -------------------------\n",
    "class SketchRNNVAE(nn.Module):\n",
    "    def __init__(self, hps):\n",
    "        super().__init__()\n",
    "        self.hps = hps\n",
    "        in_dim = 5  # input vector size (dx, dy, p1, p2, p3)\n",
    "        self.enc_rnn_size = hps[\"enc_rnn_size\"]\n",
    "        self.dec_rnn_size = hps[\"dec_rnn_size\"]\n",
    "        self.z_size = hps[\"z_size\"]\n",
    "        self.M = hps[\"num_mixture\"]\n",
    "\n",
    "        # Encoder: single-layer bidirectional LSTM (batch_first)\n",
    "        self.encoder = nn.LSTM(input_size=in_dim, hidden_size=self.enc_rnn_size,\n",
    "                               num_layers=1, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Mu and logvar\n",
    "        self.fc_mu = nn.Linear(self.enc_rnn_size * 2, self.z_size)\n",
    "        self.fc_logvar = nn.Linear(self.enc_rnn_size * 2, self.z_size)\n",
    "\n",
    "        # Decoder initial state from z -> produce 2 * dec_rnn_size (h0 and c0 concatenated)\n",
    "        self.z_to_init = nn.Linear(self.z_size, 2 * self.dec_rnn_size)\n",
    "        # tanh activation (mirrors Keras initial_state with tanh)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Decoder LSTM: we will feed (decoder_input concat tile(z))\n",
    "        self.decoder = nn.LSTM(input_size=in_dim + self.z_size, hidden_size=self.dec_rnn_size,\n",
    "                               num_layers=1, batch_first=True)\n",
    "\n",
    "        # Output dense: pen logits (3) + M * 6 (pi, mu1, mu2, sigma1, sigma2, rho)\n",
    "        self.n_out = 3 + self.M * 6\n",
    "        self.output_layer = nn.Linear(self.dec_rnn_size, self.n_out)\n",
    "\n",
    "    def encode(self, x, lengths=None):\n",
    "        # x: (B, T, 5)\n",
    "        out, (h_n, c_n) = self.encoder(x)  # out (B, T, 2*enc_rnn_size)\n",
    "        # h_n shape: (num_layers * num_directions, B, hidden)\n",
    "        if h_n.size(0) == 2:\n",
    "            # single layer, bidirectional -> two direction hidden states\n",
    "            h_forward = h_n[0]   # (B, hidden)\n",
    "            h_backward = h_n[1]  # (B, hidden)\n",
    "            h = torch.cat([h_forward, h_backward], dim=1)  # (B, 2*enc_rnn_size)\n",
    "        else:\n",
    "            # fallback: use last output timestep\n",
    "            h = out[:, -1, :]\n",
    "\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, decoder_inputs, z, initial_states=None):\n",
    "        # decoder_inputs: (B, T_dec, 5)\n",
    "        # z: (B, z_size)\n",
    "        # initial_states: tuple (h0, c0) each (1, B, dec_rnn_size) optional\n",
    "        B, T, _ = decoder_inputs.shape\n",
    "        # tile z to (B, T, z_size)\n",
    "        z_tile = z.unsqueeze(1).expand(-1, T, -1)\n",
    "        dec_in = torch.cat([decoder_inputs, z_tile], dim=2)  # (B, T, 5+z_size)\n",
    "        if initial_states is None:\n",
    "            # compute from z\n",
    "            init = self.tanh(self.z_to_init(z))  # (B, 2*dec_rnn_size)\n",
    "            h0 = init[:, :self.dec_rnn_size].unsqueeze(0).contiguous()  # (1, B, dec_rnn_size)\n",
    "            c0 = init[:, self.dec_rnn_size:].unsqueeze(0).contiguous()\n",
    "            initial_states = (h0, c0)\n",
    "        out, (h_n, c_n) = self.decoder(dec_in, initial_states)\n",
    "        y = self.output_layer(out)  # (B, T, n_out)\n",
    "        return y, (h_n, c_n)\n",
    "\n",
    "    def forward(self, encoder_inputs, decoder_inputs):\n",
    "        # encoder_inputs: (B, T_enc, 5) for encoder\n",
    "        # decoder_inputs: (B, T_dec, 5) for teacher forcing\n",
    "        mu, logvar = self.encode(encoder_inputs)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        y, _ = self.decode(decoder_inputs, z)\n",
    "        return y, mu, logvar\n",
    "\n",
    "    # helper to extract MDN parameters from network output\n",
    "    def get_mixture_coeffs(self, out):\n",
    "        # out: (B, T, n_out)\n",
    "        B, T, _ = out.shape\n",
    "        pen_logits = out[:, :, :3]  # (B,T,3)\n",
    "\n",
    "        md_params = out[:, :, 3:]  # (B, T, M*6)\n",
    "        md_params = md_params.view(B, T, self.M, 6)  # (B,T,M,6)\n",
    "\n",
    "        # convention used here: md_params[..., 0] = pi logits\n",
    "        z_pi_logits = md_params[..., 0]   # (B,T,M)\n",
    "        z_mu1 = md_params[..., 1]\n",
    "        z_mu2 = md_params[..., 2]\n",
    "        z_logsigma1 = md_params[..., 3]\n",
    "        z_logsigma2 = md_params[..., 4]\n",
    "        z_rho = md_params[..., 5]\n",
    "\n",
    "        # apply transforms\n",
    "        z_pi = F.softmax(z_pi_logits, dim=-1)        # (B,T,M)\n",
    "        z_sigma1 = torch.exp(z_logsigma1)           # positive\n",
    "        z_sigma2 = torch.exp(z_logsigma2)           # positive\n",
    "        z_rho = torch.tanh(z_rho)                   # in (-1,1)\n",
    "        z_pen = F.softmax(pen_logits, dim=-1)       # (B,T,3)\n",
    "\n",
    "        return {\n",
    "            \"pi\": z_pi,\n",
    "            \"mu1\": z_mu1,\n",
    "            \"mu2\": z_mu2,\n",
    "            \"sigma1\": z_sigma1,\n",
    "            \"sigma2\": z_sigma2,\n",
    "            \"rho\": z_rho,\n",
    "            \"pen\": z_pen,\n",
    "            \"pen_logits\": pen_logits,\n",
    "            \"pi_logits\": z_pi_logits\n",
    "        }\n",
    "\n",
    "    # evaluate 2D normal pdf for all mixtures (not used directly in loss below)\n",
    "    @staticmethod\n",
    "    def bivariate_normal_pdf(x1, x2, mu1, mu2, s1, s2, rho, eps=1e-6):\n",
    "        x1_exp = x1.unsqueeze(-1)\n",
    "        x2_exp = x2.unsqueeze(-1)\n",
    "        norm1 = x1_exp - mu1\n",
    "        norm2 = x2_exp - mu2\n",
    "        s1s2 = s1 * s2 + eps\n",
    "        z = (norm1 / (s1 + eps)) ** 2 + (norm2 / (s2 + eps)) ** 2 - 2 * (rho * norm1 * norm2) / (s1s2)\n",
    "        neg_rho = 1 - rho ** 2\n",
    "        exp_term = torch.exp(-z / (2 * (neg_rho + eps)))\n",
    "        denom = 2 * math.pi * s1s2 * torch.sqrt(neg_rho + eps)\n",
    "        pdf = exp_term / (denom + eps)\n",
    "        return pdf\n",
    "\n",
    "# -------------------------\n",
    "# Loss helpers\n",
    "# -------------------------\n",
    "def kl_loss_from_mu_logvar(mu, logvar, kl_tolerance):\n",
    "    # mu, logvar: (B, z_size)\n",
    "    kl = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    kl = torch.sum(kl, dim=1)  # sum over latent dims\n",
    "    kl = torch.mean(kl)        # average over batch\n",
    "    return torch.clamp(kl, min=kl_tolerance)\n",
    "\n",
    "def mdn_pen_loss(y_true, y_pred, model):\n",
    "    B, T, _ = y_true.shape\n",
    "    comps = model.get_mixture_coeffs(y_pred)\n",
    "    pi_logits = comps[\"pi_logits\"]\n",
    "    mu1, mu2 = comps[\"mu1\"], comps[\"mu2\"]\n",
    "    s1, s2 = comps[\"sigma1\"], comps[\"sigma2\"]\n",
    "    rho = comps[\"rho\"]\n",
    "    pen_logits = comps[\"pen_logits\"]\n",
    "\n",
    "    x1 = y_true[:, :, 0]\n",
    "    x2 = y_true[:, :, 1]\n",
    "    pen_true = y_true[:, :, 2:5]\n",
    "\n",
    "    # Expand targets to match mixtures\n",
    "    x1e = x1.unsqueeze(-1)\n",
    "    x2e = x2.unsqueeze(-1)\n",
    "\n",
    "    # Equation (24–25) from paper\n",
    "    norm1 = (x1e - mu1) / (s1 + 1e-6)\n",
    "    norm2 = (x2e - mu2) / (s2 + 1e-6)\n",
    "    z = norm1**2 + norm2**2 - 2 * rho * norm1 * norm2\n",
    "    neg_rho = 1 - rho**2 + 1e-6\n",
    "\n",
    "    log_component = -torch.log(2 * math.pi * s1 * s2 * torch.sqrt(neg_rho) + 1e-8) - z / (2 * neg_rho)\n",
    "\n",
    "    # Mix with pi\n",
    "    log_pi = F.log_softmax(pi_logits, dim=-1)\n",
    "    log_mix = torch.logsumexp(log_pi + log_component, dim=-1)\n",
    "\n",
    "    gmm_loss = -log_mix\n",
    "\n",
    "    # Mask beyond end-of-sequence\n",
    "    fs = 1.0 - pen_true[:, :, 2]\n",
    "    gmm_loss = gmm_loss * fs\n",
    "\n",
    "    # Pen loss\n",
    "    pen_target = torch.argmax(pen_true, dim=-1)\n",
    "    pen_loss_all = F.cross_entropy(\n",
    "        pen_logits.reshape(-1, 3),\n",
    "        pen_target.view(-1),\n",
    "        reduction=\"none\"\n",
    "    ).view(B, T)\n",
    "    pen_loss = pen_loss_all * fs\n",
    "\n",
    "    total = gmm_loss + pen_loss\n",
    "    denom = torch.sum(fs) + 1e-8\n",
    "    recon_loss = torch.sum(total) / denom\n",
    "    return recon_loss, gmm_loss.mean().item(), pen_loss.mean().item()\n",
    "\n",
    "# -------------------------\n",
    "# Data normalization helper (was missing)\n",
    "# -------------------------\n",
    "def normalize_strokes(batch):\n",
    "    \"\"\"\n",
    "    Normalize dx,dy across the batch by their standard deviation.\n",
    "    Expects batch shape (B, T, 5) where [:,:,0:2] = dx,dy and [:,:,2:5] is one-hot pen.\n",
    "    \"\"\"\n",
    "    coords = batch[:, :, :2]\n",
    "    # compute std across batch & time for dx/dy\n",
    "    std = coords.reshape(-1, 2).std(dim=0)\n",
    "    # handle zero std\n",
    "    std_safe = std.clone()\n",
    "    std_safe[std_safe == 0] = 1.0\n",
    "    coords = coords / std_safe.view(1, 1, 2)\n",
    "    batch = batch.clone()\n",
    "    batch[:, :, :2] = coords\n",
    "    # Ensure pen states are still floats (no change). If they are not one-hot, we leave as-is.\n",
    "    return batch\n",
    "\n",
    "# -------------------------\n",
    "# Example training loop\n",
    "# -------------------------\n",
    "hps = default_hparams()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SketchRNNVAE(hps).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hps[\"learning_rate\"])\n",
    "\n",
    "# DataLoader: handle datasets that yield single-tensor items or tuples\n",
    "dataloader = DataLoader(training_data, batch_size=hps[\"batch_size\"], shuffle=True, pin_memory=True, drop_last=True)\n",
    "\n",
    "# KL weight setup\n",
    "kl_weight = hps[\"kl_weight_start\"]\n",
    "kl_decay_rate = hps[\"kl_decay_rate\"]\n",
    "\n",
    "max_seq_length = hps[\"max_seq_len\"]\n",
    "\n",
    "for epoch in range(hps[\"epochs\"]):\n",
    "    model.train()\n",
    "    total_loss, total_recon, total_kl = 0.0, 0.0, 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
    "        batch = batch.to(device).float()  # (B, T, 5)\n",
    "\n",
    "        # normalize strokes (dx, dy only)\n",
    "        batch = normalize_strokes(batch)\n",
    "\n",
    "        # shift inputs for teacher forcing:\n",
    "        # encoder gets full sequence length T\n",
    "        # decoder receives first T-1 tokens (teacher forcing), target is next-step for those T-1 outputs\n",
    "        decoder_input = batch[:, :max_seq_length - 1, :]   # (B, T-1, 5) teacher forcing input\n",
    "        target_output = batch[:, 1:max_seq_length, :]      # (B, T-1, 5) next-step targets\n",
    "\n",
    "        # forward\n",
    "        y_pred, mu, logvar = model(batch, decoder_input)\n",
    "\n",
    "        # losses\n",
    "        recon_loss, gmm_mean, pen_mean = mdn_pen_loss(target_output, y_pred, model)\n",
    "        kl = kl_loss_from_mu_logvar(mu, logvar, hps[\"kl_tolerance\"])\n",
    "\n",
    "        # anneal KL weight (approach hps[\"kl_weight\"] gradually)\n",
    "        kl_weight = min(hps[\"kl_weight\"], kl_weight / hps[\"kl_decay_rate\"])\n",
    "        loss = recon_loss + kl_weight * kl\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), hps[\"grad_clip\"])\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_recon += recon_loss.item()\n",
    "        total_kl += kl.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{hps['epochs']} | \"\n",
    "        f\"Loss: {total_loss/n_batches:.6f} | \"\n",
    "        f\"Recon: {total_recon/n_batches:.6f} | \"\n",
    "        f\"KL: {total_kl/n_batches:.6f} | \"\n",
    "        f\"KL_weight: {kl_weight:.6f}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
