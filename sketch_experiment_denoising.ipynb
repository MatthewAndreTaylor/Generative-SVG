{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632aa9ac",
   "metadata": {},
   "source": [
    "# Initial experiments: Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "651fca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset import QuickDrawDataset\n",
    "from utils import AbsolutePenPositionTokenizer\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0c0017c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d0dd2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading QuickDraw files: 100%|██████████| 1/1 [00:03<00:00,  3.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized data from sketch_tokenized_dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "labels = [\"cat\"]\n",
    "\n",
    "training_data = QuickDrawDataset(\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "tokenizer = AbsolutePenPositionTokenizer(bins=64, additional_tokens=[\"MASK\"])\n",
    "\n",
    "\n",
    "class SketchDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        svg_list,\n",
    "        tokenizer,\n",
    "        max_len=200,\n",
    "        cache_file=\"sketch_tokenized_dataset.pkl\",\n",
    "    ):\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.pad_id = tokenizer.vocab[\"PAD\"]\n",
    "\n",
    "        # Try to load from cache\n",
    "        try:\n",
    "            with open(cache_file, \"rb\") as f:\n",
    "                self.data = pickle.load(f)\n",
    "            print(f\"Loaded tokenized data from {cache_file}\")\n",
    "        except FileNotFoundError:\n",
    "            for svg in tqdm(svg_list, desc=\"Tokenizing SVGs\"):\n",
    "                tokens = tokenizer.encode(svg)\n",
    "                # Truncate + pad\n",
    "                tokens = tokens[:max_len]\n",
    "                tokens = tokens + [self.pad_id] * (max_len - len(tokens))\n",
    "                self.data.append(tokens)\n",
    "\n",
    "            with open(cache_file, \"wb\") as f:\n",
    "                pickle.dump(self.data, f)\n",
    "            print(f\"Saved tokenized data to {cache_file}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "dataset = SketchDataset(training_data, tokenizer, max_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d81d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_input(input_ids, tokenizer, mask_prob=0.2, dropout_prob=0.05):\n",
    "    mask_token = tokenizer.vocab.get(\"MASK\", None)\n",
    "    pad_token = tokenizer.vocab[\"PAD\"]\n",
    "    assert mask_token is not None, \"Tokenizer must have a [MASK] token\"\n",
    "\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    device = input_ids.device\n",
    "\n",
    "    rand = torch.rand(batch_size, seq_len, device=device)\n",
    "    mask = (rand < mask_prob) & (input_ids != pad_token)\n",
    "    dropout = (rand < dropout_prob) & (input_ids != pad_token)\n",
    "\n",
    "    # Create masked input\n",
    "    corrupted = input_ids.clone()\n",
    "    corrupted[mask] = mask_token\n",
    "    # Remove some tokens (simulate missing strokes or words)\n",
    "    corrupted[dropout] = pad_token\n",
    "    return corrupted\n",
    "\n",
    "\n",
    "class SketchAutoencoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=6, max_len=200):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=4 * d_model,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,  # much easier shape handling\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(0, seq_len, device=x.device).unsqueeze(0)\n",
    "        x = self.embed(x) + self.pos_embed(pos)\n",
    "        h = self.encoder(x)\n",
    "        h = self.norm(h)\n",
    "        logits = self.fc_out(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95801f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 805/805 [01:44<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | DAE Loss: 1.7864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 805/805 [01:45<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | DAE Loss: 1.0907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 805/805 [01:45<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | DAE Loss: 0.9326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 805/805 [01:44<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | DAE Loss: 0.8775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 805/805 [01:45<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | DAE Loss: 0.8460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 805/805 [01:45<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | DAE Loss: 0.8257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 805/805 [01:45<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | DAE Loss: 0.8086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 805/805 [01:45<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | DAE Loss: 0.7967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 805/805 [01:45<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | DAE Loss: 0.7867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20:   6%|▋         | 51/805 [00:06<01:40,  7.48it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     29\u001b[39m dataloader = DataLoader(dataset, batch_size=\u001b[32m128\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     30\u001b[39m model = SketchAutoencoder(\n\u001b[32m     31\u001b[39m     vocab_size=\u001b[38;5;28mlen\u001b[39m(tokenizer.vocab), d_model=\u001b[32m256\u001b[39m, nhead=\u001b[32m8\u001b[39m, num_layers=\u001b[32m6\u001b[39m\n\u001b[32m     32\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mtrain_denoising_autoencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mtrain_denoising_autoencoder\u001b[39m\u001b[34m(model, dataloader, tokenizer, epochs, lr, device)\u001b[39m\n\u001b[32m     22\u001b[39m     loss.backward()\n\u001b[32m     23\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | DAE Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def train_denoising_autoencoder(\n",
    "    model, dataloader, tokenizer, epochs=10, lr=1e-4, device=\"cuda\"\n",
    "):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    pad_token_id = tokenizer.vocab[\"PAD\"]\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "    vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for clean_ids in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            clean_ids = clean_ids.to(device)\n",
    "            noisy_ids = corrupt_input(clean_ids, tokenizer)\n",
    "\n",
    "            logits = model(noisy_ids)\n",
    "            loss = criterion(logits.view(-1, vocab_size), clean_ids.view(-1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | DAE Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, pin_memory=True)\n",
    "model = SketchAutoencoder(\n",
    "    vocab_size=len(tokenizer.vocab), d_model=256, nhead=8, num_layers=6\n",
    ")\n",
    "\n",
    "train_denoising_autoencoder(\n",
    "    model, dataloader, tokenizer, epochs=20, lr=1e-3, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "803f714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"sketch_dae_model_cat.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa6ae5fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTML, display\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = torch.load(\u001b[33m\"\u001b[39m\u001b[33msketch_dae_model_cat.pth\u001b[39m\u001b[33m\"\u001b[39m, map_location=\u001b[43mdevice\u001b[49m, weights_only=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      5\u001b[39m model.eval()\n\u001b[32m      6\u001b[39m device = torch.device(device)\n",
      "\u001b[31mNameError\u001b[39m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "model = torch.load(\"sketch_dae_model_cat.pth\", map_location=device, weights_only=False)\n",
    "\n",
    "model.eval()\n",
    "device = torch.device(device)\n",
    "model = model.to(device)\n",
    "\n",
    "for i in range(5):\n",
    "    clean_seq = dataset[i].unsqueeze(0).to(device)  # (1, seq_len)\n",
    "    svg_inline = f'<div style=\"display:inline-block; width: 150px; background-color: white; margin-right:10px;\"><b>Original</b><br>{tokenizer.decode(clean_seq.squeeze(0).cpu().tolist())}</div>'\n",
    "\n",
    "    for _ in range(5):\n",
    "        noisy_seq = corrupt_input(clean_seq, tokenizer, mask_prob=0.2, dropout_prob=0.0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(noisy_seq)\n",
    "            preds = torch.argmax(logits, dim=-1)  # (1, seq_len)\n",
    "\n",
    "        denoised_seq = preds.squeeze(0).cpu().tolist()\n",
    "        denoised_svg = tokenizer.decode(denoised_seq)\n",
    "        svg_inline += f'<div style=\"display:inline-block; width: 150px; background-color: white; margin-right:10px;\"><b>Generated</b><br>{denoised_svg}</div>'\n",
    "\n",
    "    display(HTML(svg_inline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661cfa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create a model with latent space and a model to sample from it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
