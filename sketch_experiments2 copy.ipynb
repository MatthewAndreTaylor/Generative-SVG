{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632aa9ac",
   "metadata": {},
   "source": [
    "# Initial experiments Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651fca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset import QuickDrawDataset\n",
    "from utils import DeltaPenPositionTokenizer\n",
    "from prepare_data import stroke_to_bezier_single, clean_svg\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0c0017c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0dd2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading QuickDraw files: 100%|██████████| 1/1 [00:00<00:00, 4476.31it/s]\n",
      "Loading QuickDraw files: 100%|██████████| 1/1 [00:04<00:00,  4.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized data from sketch_bulldozer_dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "labels = [\"bulldozer\"]\n",
    "training_data = QuickDrawDataset(labels=labels, download=True)\n",
    "tokenizer = DeltaPenPositionTokenizer(bins=32)\n",
    "\n",
    "\n",
    "class SketchDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        svg_list,\n",
    "        tokenizer,\n",
    "        max_len=200,\n",
    "        cache_file=\"sketch_bulldozer_dataset.pkl\",\n",
    "    ):\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.pad_id = tokenizer.vocab[\"PAD\"]\n",
    "\n",
    "        # Try to load from cache\n",
    "        try:\n",
    "            with open(cache_file, \"rb\") as f:\n",
    "                self.data = pickle.load(f)\n",
    "            print(f\"Loaded tokenized data from {cache_file}\")\n",
    "        except FileNotFoundError:\n",
    "            for svg in tqdm(svg_list, desc=\"Tokenizing SVGs\"):\n",
    "                # use RDP to reduce number of points in SVG\n",
    "                # svg = stroke_to_rdp(svg, epsilon=1.0)  # tuning\n",
    "                tokens = tokenizer.encode(svg)[:max_len]\n",
    "                tokens = tokens + [self.pad_id] * (max_len - len(tokens))\n",
    "                self.data.append(tokens)\n",
    "\n",
    "            with open(cache_file, \"wb\") as f:\n",
    "                pickle.dump(self.data, f)\n",
    "            print(f\"Saved tokenized data to {cache_file}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data[idx]\n",
    "        input_ids = torch.tensor(seq[:-1])\n",
    "        target_ids = torch.tensor(seq[1:])\n",
    "        return input_ids, target_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "dataset = SketchDataset(training_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5d81d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:  44%|████▍     | 575/1293 [02:31<03:08,  3.80it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# d_model => model capacity (types of drawing features it can learn)\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# nhead => model can attend to more positions in parallel\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# num layers => model learns more hierarchical abstractions (patterns, shapes , layouts)\u001b[39;00m\n\u001b[32m     68\u001b[39m model = torch.load(\u001b[33m\"\u001b[39m\u001b[33msketch_transformer_model_bulldozer_v2_deep.pth\u001b[39m\u001b[33m\"\u001b[39m, map_location=device, weights_only=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, dataloader, vocab_size, epochs, lr, device)\u001b[39m\n\u001b[32m     53\u001b[39m     loss.backward()\n\u001b[32m     54\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss/\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def generate_square_subsequent_mask(sz: int):\n",
    "    \"\"\"Causal mask to stop attention to future positions\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
    "\n",
    "\n",
    "class SketchTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=384, nhead=8, num_layers=6, max_len=200):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=4 * d_model\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len) input tokens\n",
    "        Returns: (batch, seq_len, vocab_size) logits\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.shape\n",
    "        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0)\n",
    "        x = self.embed(x) + self.pos_embed(positions)  # (batch, seq_len, d_model)\n",
    "        x = x.transpose(0, 1)  # -> (seq_len, batch, d_model)\n",
    "        mask = generate_square_subsequent_mask(seq_len).to(\n",
    "            x.device\n",
    "        )  # causal mask (seq_len, seq_len)\n",
    "        x = self.transformer(x, mask=mask)  # (seq_len, batch, d_model)\n",
    "        x = x.transpose(0, 1)  # back to (batch, seq_len, d_model)\n",
    "        logits = self.fc_out(x)  # (batch, seq_len, vocab_size)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, vocab_size, epochs=10, lr=1e-4, device=\"cuda\"):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for input_ids, target_ids in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "\n",
    "            # Transformer expects shape (seq_len, batch, d_model)\n",
    "            logits = model(input_ids)  # (seq_len, batch, vocab_size)\n",
    "            loss = criterion(logits.view(-1, vocab_size), target_ids.view(-1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, pin_memory=True)\n",
    "model = SketchTransformer(\n",
    "    vocab_size=len(tokenizer.vocab), d_model=512, nhead=8, num_layers=6\n",
    ")\n",
    "\n",
    "# d_model => model capacity (types of drawing features it can learn)\n",
    "# nhead => model can attend to more positions in parallel\n",
    "# num layers => model learns more hierarchical abstractions (patterns, shapes , layouts)\n",
    "model = torch.load(\n",
    "    \"sketch_transformer_model_bulldozer_v2_deep.pth\",\n",
    "    map_location=device,\n",
    "    weights_only=False,\n",
    ")\n",
    "\n",
    "train_model(\n",
    "    model,\n",
    "    dataloader,\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    epochs=15,\n",
    "    lr=1e-4,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e85165ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"sketch_transformer_model_bulldozer_v2_deep.pth\")\n",
    "# model = torch.load(\"sketch_transformer_model_bulldozer_v2_deep.pth\", map_location=device, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1502569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display:inline-block; width: 150px; background-color: white; margin-right:10px;\"><b>Generated</b><br><svg viewBox=\"0 0 32 32\" xmlns=\"http://www.w3.org/2000/svg\"><g stroke-width=\"0.3\"><path d=\"M3,8C2.6762634554107914,11.5808605639407 -0.05411962722830366,12.200992551195727 -0.9638193761663261,14.963819376166326C-1.643097451598391,17.026838180567438 2.2334207252251246,17.11651401315366 3.2819323724908176,17C7.406627183860321,16.54165054173693 9.024255881564846,9 4,9M7,13C7.666666666666668,13 8.333333333333332,13 9,13M6,15C7.333333333333333,15 8.666666666666668,15 10,15M10,12C10,13.180729692252918 9.555986730430757,15.41395493222927 11.03129228683537,16C12.334772105987543,16.51778963051862 15.145679938927602,15.040358179825708 15,13.46252781886066C14.775372590257849,11.029633148495385 10.010709685168578,11.989290314831422 9,13M15,13C9.888282384867974,18.111717615132026 20.696211310852934,13 14,13M19,13C16.353777778715262,15.64622222128476 21.646222221284734,15.646222221284754 19,13M20,13C20,16.745120557521577 23.480210470469125,13 21,13M24,13C21.235767741346088,13 25.887293160260086,14.887293160260088 23,12\" fill=\"none\" stroke=\"#000000\" /></g></svg></div><div style=\"display:inline-block; width: 150px; background-color: white; margin-right:10px;\"><b>Generated</b><br><svg viewBox=\"0 0 32 32\" xmlns=\"http://www.w3.org/2000/svg\"><g stroke-width=\"0.3\"><path d=\"M0,7C5.605234093063645,7 6.7098846122645375,16.645057693867738 2,19M6,11C7.333333333333333,11 8.666666666666668,11 10,11M6,13C7.000000000000002,13 7.999999999999999,13 9,13M9,10C9,11.333333333333334 9,12.666666666666668 9,14M9,9C10.333333333333334,9 11.666666666666666,9 13,9C13,5.980084672934629 12.786991102020844,4 16.454701064473795,4C16.809623993268854,4 18.76684838940728,3.7749398591416057 19,4.018290745131655C19.61404117256909,4.659193302052367 19,9.023265851340163 19,10C21.604157573930213,10 28.528974079429652,10.23961539318191 24.841795612052547,15.079102193973727C23.324418993793007,17.07068578041208 17.188288900005492,16 15,16C15,20.151039476521422 19.243685735107878,15 15,15M24,15C24,18.29428773237698 27.804346403508795,15 23,15M12,10C12,11.70397795165056 13.699202612277833,10.699202612277833 12,9M13,9C13,8.666666666666666 13,8.333333333333334 13,8\" fill=\"none\" stroke=\"#000000\" /></g></svg></div><div style=\"display:inline-block; width: 150px; background-color: white; margin-right:10px;\"><b>Generated</b><br><svg viewBox=\"0 0 32 32\" xmlns=\"http://www.w3.org/2000/svg\"><g stroke-width=\"0.3\"><path d=\"M3,10C-0.7085665491858459,11.854283274592921 1.6385341051321038,16.714588390404728 5.183038340089713,14.816961659910287C9.276833015686137,12.625260454153228 5.226321771656492,6.7736782283435115 2,10M17,11C15.03725648781645,11 10.701417613401802,14.373472068284256 14.48179205521163,15C19.00250447970384,15.749225413724574 18.065305831578335,12.532652915789168 15,11M3,12C3.9999999999999996,12 5.000000000000001,12 6,12M9,12C9.666666666666666,12 10.333333333333332,12 11,12M15,12C16.528841654485444,10.471158345514556 16.228399264064485,8.261111900056484 14.187431300062558,7.187431300062558C9.898064369016225,4.9309480326379935 1,6.538037279855795 1,12M0,10C0,8.13050465808686 -0.7189594064567522,4.416900929134095 0.23671838181485974,2.76328161818514C1.324648034262617,0.8808255181064606 8.06106976005346,-0.329988945639613 9,2.0499690384803895C9.67971233639491,3.772873183095046 8.535252667712195,6.394241996863412 8,8M8,5C8.999999999999998,5 10.000000000000002,5 11,5M11,2C11,3.333333333333333 11,4.666666666666667 11,6M12,2C13.333333333333329,2.333333333333332 14.66666666666667,2.6666666666666674 16,3M12,3C13.000000000000002,3.3333333333333366 13.999999999999996,3.6666666666666665 15,4C15,4.333333333333333 15,4.666666666666667 15,5M14,5C15,5 16,5 17,5C17,5.999999999999999 17,7.000000000000001 17,8M17,6C17.333333333333332,6 17.666666666666664,6 18,6C18,6.333333333333334 18,6.666666666666667 18,7C18.33333333333334,7.333333333333328 18.66666666666666,7.666666666666671 19,8M19,7C19.333333333333336,7 19.666666666666664,7 20,7M19,7C18.666666666666668,7 18.333333333333332,7 18,7\" fill=\"none\" stroke=\"#000000\" /></g></svg></div><div style=\"display:inline-block; width: 150px; background-color: white; margin-right:10px;\"><b>Generated</b><br><svg viewBox=\"0 0 32 32\" xmlns=\"http://www.w3.org/2000/svg\"><g stroke-width=\"0.3\"><path d=\"M1,12C1,10.441462171420923 4.18935512999224,8.186717290770025 5.773206941963209,9C7.2475536993450635,9.757053605725787 4.493728922683598,13.493728922683601 2,11M9,11C9,10.233616567262082 10.392664756580388,9.138476993754159 11,10.099973417716404C12.461809596374357,12.414221956157556 9.068878114476446,12.068878114476446 8,11M-3,10C-1.7364245438912844,10 3.240893004245681,9.221881930649333 3,11.52539348282985C2.8262557406203674,13.186802911714114 -3.1448702406411426,11.427564879679428 -4,11M7,11C8.000000000000004,11 8.999999999999998,11 10,11M10,10C10,9.666666666666666 10,9.333333333333334 10,9M8,9C8.333333333333332,9 8.666666666666668,9 9,9M7,9C7.333333333333334,9 7.666666666666666,9 8,9C7.666666666666666,9 7.333333333333335,9 7,9\" fill=\"none\" stroke=\"#000000\" /></g></svg></div><div style=\"display:inline-block; width: 150px; background-color: white; margin-right:10px;\"><b>Generated</b><br><svg viewBox=\"0 0 32 32\" xmlns=\"http://www.w3.org/2000/svg\"><g stroke-width=\"0.3\"><path d=\"M0,14C1.0994028962435753,14 3.2678151761701577,13.581313023894158 4.206622957203073,14.206622957203072C6.800519879495699,15.934335002478647 4.142130393879835,19 2,19M3,13C6.333333333333332,13 9.666666666666668,13 13,13C11.153360959622589,14.846639040377411 11.258595463258402,19.104738937612897 14.83152578280123,18C17.063533679820964,17.309870103144725 16.124012158273707,14 14,14M23,14C17.602646042671413,14.04559365090628 18.831177684833165,21.519101457412965 22.91078639403893,20C25.212385000421154,19.1429663856131 26.169072224522118,14.042478091539268 23,14M11,14C11,8.536608334133941 14.497817568714144,9 18.830012775938304,9C19.829518352823587,9 22.401947797209164,8.509050569203115 23.133489385698127,9.400468157094377C24.245470665159335,10.755469274617997 24,13.3940476026212 24,15M13,9C13,7.625518634597016 12.66531823284121,5.7331774675046985 13,4.406498545334577C13.64738960256999,1.8402453925173115 17.681254212148133,1.2918979077726336 19.803701875583723,2C22.855639528032448,3.018203404624021 23,7.487724205342509 23,10C23.666666666666664,10.333333333333332 24.333333333333336,10.666666666666668 25,11C26.333333333333332,11.666666666666666 27.666666666666668,12.333333333333334 29,13M28,13C28.666666666666664,13 29.333333333333332,13 30,13C30.33333333333334,13 30.66666666666666,13 31,13\" fill=\"none\" stroke=\"#000000\" /></g></svg></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def top_p_filtering(logits, p=0.9):\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "    sorted_indices_to_remove = cumulative_probs > p\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = False\n",
    "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "    logits[:, indices_to_remove] = -float(\"Inf\")\n",
    "    return logits\n",
    "\n",
    "\n",
    "def top_k_filtering(logits, k):\n",
    "    if k <= 0:\n",
    "        return logits\n",
    "    top_k = min(k, logits.size(-1))\n",
    "    values, _ = torch.topk(logits, top_k)\n",
    "    min_values = values[:, -1].unsqueeze(-1)\n",
    "    logits[logits < min_values] = -float(\"Inf\")\n",
    "    return logits\n",
    "\n",
    "\n",
    "def sample_sequence_feat(\n",
    "    model,\n",
    "    start_tokens,\n",
    "    max_len=200,\n",
    "    temperature=0.6,\n",
    "    top_k=20,\n",
    "    top_p=0.9,\n",
    "    greedy=False,\n",
    "    eos_id=None,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    tokens = list(start_tokens)\n",
    "    tokens_tensor = torch.tensor([tokens], device=device, dtype=torch.long)\n",
    "\n",
    "    for _ in range(max_len - len(tokens)):\n",
    "        with torch.no_grad():\n",
    "            logits = model(tokens_tensor)\n",
    "            next_logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            # top-k / top-p filtering\n",
    "            next_logits = top_k_filtering(next_logits, top_k)\n",
    "            next_logits = top_p_filtering(next_logits, top_p)\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            if greedy:\n",
    "                next_token = torch.argmax(probs, dim=-1).item()\n",
    "            else:\n",
    "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        tokens.append(next_token)\n",
    "        if eos_id is not None and next_token == eos_id:\n",
    "            break\n",
    "\n",
    "        next_token_tensor = torch.tensor([[next_token]], device=device)\n",
    "        tokens_tensor = torch.cat([tokens_tensor, next_token_tensor], dim=1)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "start_token = tokenizer.vocab[\"START\"]\n",
    "eos_token = tokenizer.vocab.get(\"END\", None)\n",
    "generations_inline = \"\"\n",
    "generations = []\n",
    "\n",
    "for i in range(5):\n",
    "    generated = sample_sequence_feat(\n",
    "        model,\n",
    "        start_tokens=[start_token],\n",
    "        max_len=200,\n",
    "        greedy=False,\n",
    "        eos_id=eos_token,\n",
    "        device=device,\n",
    "    )\n",
    "    decoded_sketch = tokenizer.decode(generated, stroke_width=0.3)\n",
    "\n",
    "    decoded_sketch = stroke_to_bezier_single(decoded_sketch)\n",
    "    decoded_sketch = clean_svg(decoded_sketch)\n",
    "\n",
    "    # print(\"Generated token sequence:\", generated)\n",
    "    # print(\"Decoded sketch:\", decoded_sketch)\n",
    "    generations_inline += f'<div style=\"display:inline-block; width: 150px; background-color: white; margin-right:10px;\"><b>Generated</b><br>{decoded_sketch}</div>'\n",
    "    generations.append((generated, decoded_sketch))\n",
    "\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "display(HTML(generations_inline))\n",
    "\n",
    "# temp<=0.5 fairly deterministic\n",
    "\n",
    "# temp=0.8, top_k=20, top_p=0.9 more variety but still coherent\n",
    "\n",
    "# *note important features are usually preseved, but sketches are disorganized (number of curves hueristic does not work well)*\n",
    "# temp=1.0, top_k=20, top_p=0.75 more variety, some incoherent sequences\n",
    "\n",
    "#  *note that lower temp means less variety, notice that sequences begin to repete themselves more often*\n",
    "# temp=0.55, top_k=20, top_p=0.9 good balance\n",
    "# temp=0.6, top_k=30, top_p=0.9  good balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6baa8d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<svg viewBox=\"0 0 32 32\" xmlns=\"http://www.w3.org/2000/svg\"><g stroke-width=\"0.3\"><path d=\"M1,1C0.7408154984157409,3.591845015842592 -0.6792945534187917,8.342651239837494 0.017986217977667418,10.820137820223326C0.5591579055484708,12.74295807949461 12.073717497558519,12.845372513738523 13.44046763114847,11.55953236885153C14.199687379871902,10.84525752979562 14,7.609286420644203 14,6.856740080841096C14,6.752727727568782 14.01850823414632,2.0619400543864974 13.944974291716022,1.9449742917160215C12.53040137323382,-0.30509682911510727 4.278372225697419,1 2,1M3,15C1.4396482348577961,15 -1.9999479232739819,16.90419215754308 1.0934655498836956,18C3.9309313879904595,19.005141195989907 6.966702610306262,15 3,15M14,14C12.621294308542211,14 9.911625114048926,16.697230957687715 11.805658383430288,17.805658383430288C15.453665238459607,19.9405474124557 17.74133919794931,14.87066959897466 14,13M15,8C17,9.999999999999998 18.999999999999996,11.999999999999996 21,14C22.331124476802106,10.00662656959369 24.647858361377082,7 29,7C29.906971910535763,8.81394382107152 30.061039917684287,15.60844758515047 26.305246312594896,15C23.265947749119555,14.507626332556725 24,8.985826688339161 24,7C23.66666666666666,7.333333333333327 23.33333333333334,7.666666666666672 23,8C23,8.333333333333332 23,8.666666666666668 23,9M22,10C22,9.666666666666666 22,9.333333333333334 22,9\" fill=\"none\" stroke=\"#000000\" /></g></svg>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"display:inline-block; width: 250px; background-color: white; margin-right:10px;\"><b>Generated</b><br><svg viewBox=\"0 0 32 32\" xmlns=\"http://www.w3.org/2000/svg\"><g stroke-width=\"0.5\"><path d=\"M1,1C0.7408154984157409,3.591845015842592 -0.6792945534187917,8.342651239837494 0.017986217977667418,10.820137820223326C0.5591579055484708,12.74295807949461 12.073717497558519,12.845372513738523 13.44046763114847,11.55953236885153C14.199687379871902,10.84525752979562 14,7.609286420644203 14,6.856740080841096C14,6.752727727568782 14.01850823414632,2.0619400543864974 13.944974291716022,1.9449742917160215C12.53040137323382,-0.30509682911510727 4.278372225697419,1 2,1M3,15C1.4396482348577961,15 -1.9999479232739819,16.90419215754308 1.0934655498836956,18C3.9309313879904595,19.005141195989907 6.966702610306262,15 3,15M14,14C12.621294308542211,14 9.911625114048926,16.697230957687715 11.805658383430288,17.805658383430288C15.453665238459607,19.9405474124557 17.74133919794931,14.87066959897466 14,13M15,8C17,9.999999999999998 18.999999999999996,11.999999999999996 21,14C22.331124476802106,10.00662656959369 24.647858361377082,7 29,7C29.906971910535763,8.81394382107152 30.061039917684287,15.60844758515047 26.305246312594896,15C23.265947749119555,14.507626332556725 24,8.985826688339161 24,7C23.66666666666666,7.333333333333327 23.33333333333334,7.666666666666672 23,8C23,8.333333333333332 23,8.666666666666668 23,9M22,10C22,9.666666666666666 22,9.333333333333334 22,9\" fill=\"none\" stroke=\"#000000\" /></g></svg></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, decoded_sketch = generations[0]\n",
    "\n",
    "print(decoded_sketch)\n",
    "\n",
    "s = f'<div style=\"display:inline-block; width: 250px; background-color: white; margin-right:10px;\"><b>Generated</b><br><svg viewBox=\"0 0 32 32\" xmlns=\"http://www.w3.org/2000/svg\"><g stroke-width=\"0.5\"><path d=\"M1,1C0.7408154984157409,3.591845015842592 -0.6792945534187917,8.342651239837494 0.017986217977667418,10.820137820223326C0.5591579055484708,12.74295807949461 12.073717497558519,12.845372513738523 13.44046763114847,11.55953236885153C14.199687379871902,10.84525752979562 14,7.609286420644203 14,6.856740080841096C14,6.752727727568782 14.01850823414632,2.0619400543864974 13.944974291716022,1.9449742917160215C12.53040137323382,-0.30509682911510727 4.278372225697419,1 2,1M3,15C1.4396482348577961,15 -1.9999479232739819,16.90419215754308 1.0934655498836956,18C3.9309313879904595,19.005141195989907 6.966702610306262,15 3,15M14,14C12.621294308542211,14 9.911625114048926,16.697230957687715 11.805658383430288,17.805658383430288C15.453665238459607,19.9405474124557 17.74133919794931,14.87066959897466 14,13M15,8C17,9.999999999999998 18.999999999999996,11.999999999999996 21,14C22.331124476802106,10.00662656959369 24.647858361377082,7 29,7C29.906971910535763,8.81394382107152 30.061039917684287,15.60844758515047 26.305246312594896,15C23.265947749119555,14.507626332556725 24,8.985826688339161 24,7C23.66666666666666,7.333333333333327 23.33333333333334,7.666666666666672 23,8C23,8.333333333333332 23,8.666666666666668 23,9M22,10C22,9.666666666666666 22,9.333333333333334 22,9\" fill=\"none\" stroke=\"#000000\" /></g></svg></div>'\n",
    "\n",
    "display(HTML(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db551456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note many sketches have missing parts or incomplete shapes (step 1: get a base sketch) : check the number of paths\n",
    "\n",
    "# psuedo hueuristic: count number of curves in SVG\n",
    "\n",
    "from prepare_data import count_curves\n",
    "\n",
    "# sort generations by number of curves\n",
    "generations_inline = \"\"\n",
    "\n",
    "generations_sorted = sorted(generations, key=lambda x: count_curves(x[1]), reverse=True)\n",
    "for sketch in generations_sorted:\n",
    "    generations_inline += f'<div style=\"display:inline-block; width: 150px; background-color: white; margin-right:10px;\"><b>Generated</b><br>{sketch[1]}</div>'\n",
    "\n",
    "display(HTML(generations_inline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d838d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sketch from the dataset, remove tokens and let the model complete it\n",
    "selected_sketch = training_data[38946]\n",
    "\n",
    "# tokenize and remove some tokens from the end\n",
    "selected_tokens = tokenizer.encode(selected_sketch)\n",
    "\n",
    "selected_tokens_partial = selected_tokens[\n",
    "    : int(len(selected_tokens) * 0.5)\n",
    "]  # remove 50%\n",
    "destroyed_sketch = tokenizer.decode(selected_tokens_partial)\n",
    "\n",
    "comparison_inline = f\"\"\"<div style=\"display:inline-block; width: 150px; background-color: white; margin-right:10px;\"><b>Original</b><br>{selected_sketch}</div>\n",
    "<div style=\"display:inline-block; width: 150px; background-color: white; margin-right:10px;\"><b>Partial</b><br>{destroyed_sketch}</div>\"\"\"\n",
    "\n",
    "for i in range(5):\n",
    "    generated_completion = sample_sequence_feat(\n",
    "        model,\n",
    "        start_tokens=selected_tokens_partial,\n",
    "        max_len=200,\n",
    "        temperature=0.6,\n",
    "        top_k=20,\n",
    "        top_p=0.9,\n",
    "        greedy=False,\n",
    "        eos_id=eos_token,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    generated_sketch = tokenizer.decode(generated_completion, stroke_width=0.3)\n",
    "    generated_sketch = stroke_to_bezier_single(generated_sketch)\n",
    "    generated_sketch = clean_svg(generated_sketch)\n",
    "    comparison_inline += f'<div style=\"display:inline-block; width: 150px; background-color: white; margin-right:10px;\"><b>Completed {i}</b><br>{generated_sketch}</div>'\n",
    "\n",
    "display(HTML(comparison_inline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8683e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, tokens):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(tokens[:, :-1])\n",
    "        target = tokens[:, 1:]\n",
    "        loss = F.cross_entropy(\n",
    "            logits.transpose(1, 2),  # (batch, vocab, seq_len)\n",
    "            target,\n",
    "            reduction=\"none\",\n",
    "        )\n",
    "\n",
    "        loss = loss.mean(dim=1)\n",
    "        perplexity = torch.exp(loss)\n",
    "        return perplexity\n",
    "\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "sketch_perplexities = []\n",
    "\n",
    "for i in tqdm(range(20), desc=\"Computing perplexities\"):\n",
    "    sketch = training_data[i]\n",
    "    tokens = tokenizer.encode(sketch)\n",
    "    perplexity = compute_perplexity(model, torch.tensor([tokens], device=device))\n",
    "    decoded_sketch = tokenizer.decode(tokens, stroke_width=0.3)\n",
    "    sketch_perplexities.append((perplexity.item(), decoded_sketch))\n",
    "\n",
    "# sort by perplexity\n",
    "sketch_perplexities.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# sort normalized by length\n",
    "# sketch_perplexities.sort(key=lambda x: x[0] / len(x[1]), reverse=True)\n",
    "\n",
    "sketches_inline = \"\"\n",
    "for perp, sketch in sketch_perplexities:\n",
    "    sketches_inline += f'<div style=\"display:inline-block; width: 150px; background-color: white; margin-right:10px;\"><b>Perplexity: {perp:.2f}</b><br>{sketch}</div>'\n",
    "display(HTML(sketches_inline))\n",
    "\n",
    "# Sorting by perplexity does seem to highlight some of the worse sketches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
