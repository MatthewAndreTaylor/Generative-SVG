{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ee6e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "\n",
    "path_to = \"../logs/sketch_transformer_experiment_0/1762736869.7878423/events.out.tfevents.1762736869.DESKTOP-6UC3P7J.28764.1\"\n",
    "def extract_dictionary():\n",
    "    for e in tf.train.summary_iterator(path_to):\n",
    "        for v in e.summary.value:\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0038c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import tensor_util  # for tensor values\n",
    "\n",
    "path = r\"../logs/sketch_transformer_experiment_0/1762736869.7878423/events.out.tfevents.1762736869.DESKTOP-6UC3P7J.28764.1\"\n",
    "# On Windows, make sure the path is correct; prefer an absolute \"C:\\\\...\" path.\n",
    "\n",
    "def dump_events(path_or_dir):\n",
    "    files = []\n",
    "    if os.path.isdir(path_or_dir):\n",
    "        files = glob.glob(os.path.join(path_or_dir, \"events.out.tfevents.*\"))\n",
    "    elif os.path.isfile(path_or_dir):\n",
    "        files = [path_or_dir]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Path not found: {path_or_dir}\")\n",
    "\n",
    "    for f in files:\n",
    "        for e in tf.compat.v1.train.summary_iterator(f):\n",
    "            for v in e.summary.value:\n",
    "                # Print scalars and tensors nicely\n",
    "                if v.HasField(\"simple_value\"):\n",
    "                    print(e.step, v.tag, v.simple_value)\n",
    "                elif v.HasField(\"tensor\"):\n",
    "                    try:\n",
    "                        val = tensor_util.MakeNdarray(v.tensor)\n",
    "                        print(e.step, v.tag, val)\n",
    "                    except Exception:\n",
    "                        print(e.step, v.tag, \"<non-scalar>\")\n",
    "\n",
    "dump_events(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49bcf0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available scalar tags: []\n"
     ]
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "# Path to one event file or a directory containing event files\n",
    "event_path = \"../logs\"\n",
    "\n",
    "ea = event_accumulator.EventAccumulator(event_path)\n",
    "ea.Reload()  # Loads events from file\n",
    "\n",
    "# List available tags (e.g. scalars, images, histograms)\n",
    "print(\"Available scalar tags:\", ea.Tags()[\"scalars\"])\n",
    "\n",
    "# Get all scalar data for a tag\n",
    "#scalars = ea.Scalars(\"Loss/train\")\n",
    "\n",
    "# Each entry is a namedtuple: (wall_time, step, value)\n",
    "##for s in scalars:\n",
    "#    print(f\"Step {s.step} -> {s.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "edac7455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Wall time: 1762733017.8865252\n",
      "Step: 0, Wall time: 1762733020.9836898\n",
      "Step: 0, Wall time: 1762733020.984215\n",
      "Step: 0, Wall time: 1762733021.7816718\n",
      "  Tag: Targets/ValTokens, Value: 0.0\n",
      "Step: 0, Wall time: 1762733717.0144799\n",
      "  Tag: Loss/Train, Value: 2.1321747303009033\n",
      "Step: 0, Wall time: 1762733858.983716\n",
      "  Tag: Predictions/ValTokens, Value: 0.0\n",
      "Step: 0, Wall time: 1762733858.9842446\n",
      "  Tag: Loss/Val, Value: 2.1101455688476562\n",
      "Step: 0, Wall time: 1762733858.9847455\n",
      "  Tag: Accuracy/ValNextToken, Value: 0.13740158081054688\n",
      "Step: 0, Wall time: 1762733858.9895346\n",
      "  Tag: Perplexity/Val, Value: 8.249442100524902\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "file_path = \"../logs/sketch_transformer_experiment_0/events.out.tfevents.1762733017.DESKTOP-6UC3P7J.22876.8\"\n",
    "\n",
    "\n",
    "for e in tf.compat.v1.train.summary_iterator(file_path):\n",
    "    print(f\"Step: {e.step}, Wall time: {e.wall_time}\")\n",
    "    for v in e.summary.value:\n",
    "        print(f\"  Tag: {v.tag}, Value: {v.simple_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8d82a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# TF2 users: the iterator lives under compat.v1\n",
    "summary_iterator = tf.compat.v1.train.summary_iterator\n",
    "\n",
    "# Proto definitions for the HParams plugin\n",
    "from tensorboard.plugins.hparams import api_pb2\n",
    "\n",
    "def _val_to_python(v):  # tensorboard.hparams Value -> py value\n",
    "    kind = v.WhichOneof(\"kind\")\n",
    "    if kind == \"string_value\":\n",
    "        return v.string_value\n",
    "    if kind == \"bool_value\":\n",
    "        return v.bool_value\n",
    "    if kind == \"number_value\":\n",
    "        return v.number_value\n",
    "    return None\n",
    "\n",
    "def extract_hparams_from_event_file(event_path):\n",
    "    runs = []  # list of dicts: {\"run\": event_path, \"hparams\": {...}}\n",
    "    for e in summary_iterator(event_path):\n",
    "        if not e.summary.value:\n",
    "            continue\n",
    "        for v in e.summary.value:\n",
    "            md = v.metadata\n",
    "            if not md or not md.plugin_data:\n",
    "                continue\n",
    "            pd = md.plugin_data\n",
    "            # HParams plugin uses plugin_name == \"hparams\"\n",
    "            if pd.plugin_name != \"hparams\":\n",
    "                continue\n",
    "\n",
    "            hp_data = api_pb2.HParamsPluginData()\n",
    "            hp_data.ParseFromString(pd.content)\n",
    "            which = hp_data.WhichOneof(\"data\")  # \"experiment\" | \"session_start_info\" | \"session_end_info\"\n",
    "\n",
    "            if which == \"session_start_info\":\n",
    "                ss = hp_data.session_start_info\n",
    "                hps = {k: _val_to_python(val) for k, val in ss.hparams.items()}\n",
    "                runs.append({\"run\": event_path, \"group_name\": ss.group_name, \"hparams\": hps})\n",
    "    return runs\n",
    "\n",
    "def extract_hparams(logdir_or_file):\n",
    "    paths = []\n",
    "    if os.path.isdir(logdir_or_file):\n",
    "        # You can add deeper globs if you have nested runs\n",
    "        paths = glob.glob(os.path.join(logdir_or_file, \"events.out.tfevents.*\"))\n",
    "    else:\n",
    "        paths = [logdir_or_file]\n",
    "\n",
    "    all_runs = []\n",
    "    for p in paths:\n",
    "        all_runs.extend(extract_hparams_from_event_file(p))\n",
    "    return all_runs\n",
    "\n",
    "# --- usage ---\n",
    "logpath = \"../logs/sketch_transformer_experiment_0/events.out.tfevents.1762733017.DESKTOP-6UC3P7J.22876.8\"  # can be a directory or a single event file\n",
    "for run in extract_hparams(logpath):\n",
    "    print(\"RUN:\", run[\"run\"])\n",
    "    if run.get(\"group_name\"):\n",
    "        print(\" group:\", run[\"group_name\"])\n",
    "    for k, v in run[\"hparams\"].items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2382e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import tensor_util  # safe to use; stable API is limited\n",
    "\n",
    "summary_iterator = tf.compat.v1.train.summary_iterator\n",
    "\n",
    "def try_decode_tensor_to_str(t):\n",
    "    arr = tensor_util.MakeNdarray(t)\n",
    "    # Common cases: scalar bytes, 1D array of bytes/strings\n",
    "    if arr.dtype.type is np.object_:\n",
    "        # e.g., array([b'...']), pick first non-empty\n",
    "        for x in arr.flatten():\n",
    "            if isinstance(x, (bytes, bytearray)):\n",
    "                try:\n",
    "                    return x.decode(\"utf-8\", errors=\"ignore\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "            elif isinstance(x, str):\n",
    "                return x\n",
    "        return None\n",
    "    if arr.dtype.type is np.bytes_:\n",
    "        try:\n",
    "            return arr.tobytes().decode(\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            return None\n",
    "    if arr.ndim == 0:\n",
    "        return str(arr.item())\n",
    "    return str(arr)\n",
    "\n",
    "def scan_textlike_hparams(event_path):\n",
    "    hits = []\n",
    "    for e in summary_iterator(event_path):\n",
    "        for v in e.summary.value:\n",
    "            tag = v.tag.lower()\n",
    "            if (\"hparam\" in tag) or (\"hyperparam\" in tag) or (\"config\" in tag):\n",
    "                if v.HasField(\"tensor\"):\n",
    "                    s = try_decode_tensor_to_str(v.tensor)\n",
    "                    if s:\n",
    "                        # Try JSON parse, otherwise keep raw text\n",
    "                        try:\n",
    "                            hits.append((v.tag, json.loads(s)))\n",
    "                        except Exception:\n",
    "                            hits.append((v.tag, s))\n",
    "                elif v.HasField(\"simple_value\"):\n",
    "                    hits.append((v.tag, v.simple_value))\n",
    "                elif v.HasField(\"metadata\") and v.metadata.display_name:\n",
    "                    hits.append((v.tag, v.metadata.display_name))\n",
    "    return hits\n",
    "\n",
    "# --- usage ---\n",
    "event_file = \"../logs/sketch_transformer_experiment_0/1762733017.8895097/events.out.tfevents.1762733017.DESKTOP-6UC3P7J.22876.9\"  # can be a directory or a single event file\n",
    "for run in extract_hparams(logpath):\n",
    "    print(\"RUN:\", run[\"run\"])\n",
    "for tag, content in scan_textlike_hparams(event_file):\n",
    "    print(f\"[{tag}]\")\n",
    "    print(content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b614c9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'_hparams_/session_start_info'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m ea \u001b[38;5;241m=\u001b[39m event_accumulator\u001b[38;5;241m.\u001b[39mEventAccumulator(path_str, \n\u001b[0;32m      6\u001b[0m                                         size_guidance\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensors\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m} \n\u001b[0;32m      7\u001b[0m                                         )\n\u001b[0;32m      8\u001b[0m ea\u001b[38;5;241m.\u001b[39mReload()\n\u001b[1;32m---> 10\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mea\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plugin_to_tag_to_content\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_hparams_/session_start_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     11\u001b[0m hparam_data \u001b[38;5;241m=\u001b[39m HParamsPluginData\u001b[38;5;241m.\u001b[39mFromString(data)\u001b[38;5;241m.\u001b[39msession_start_info\u001b[38;5;241m.\u001b[39mhparams\n\u001b[0;32m     12\u001b[0m hparam_dict \u001b[38;5;241m=\u001b[39m {key: hparam_data[key]\u001b[38;5;241m.\u001b[39mListFields()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m hparam_data\u001b[38;5;241m.\u001b[39mkeys()}\n",
      "\u001b[1;31mKeyError\u001b[0m: '_hparams_/session_start_info'"
     ]
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "from tensorboard.plugins.hparams.plugin_data_pb2 import HParamsPluginData\n",
    "\n",
    "path_str = \"../logs/sketch_transformer_experiment_0/events.out.tfevents.1762733017.DESKTOP-6UC3P7J.22876.8\"\n",
    "ea = event_accumulator.EventAccumulator(path_str, \n",
    "                                        size_guidance={'tensors': 0} \n",
    "                                        )\n",
    "ea.Reload()\n",
    "\n",
    "data = ea._plugin_to_tag_to_content[\"hparams\"][\"_hparams_/session_start_info\"]\n",
    "hparam_data = HParamsPluginData.FromString(data).session_start_info.hparams\n",
    "hparam_dict = {key: hparam_data[key].ListFields()[0][1] for key in hparam_data.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e5f8fb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DirectoryLoader.__init__() missing 1 required positional argument: 'loader_factory'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhparams\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api_pb2\n\u001b[0;32m      4\u001b[0m run_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../logs/sketch_transformer_experiment_0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m dl \u001b[38;5;241m=\u001b[39m \u001b[43mdirectory_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDirectoryLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ev_file \u001b[38;5;129;01min\u001b[39;00m dl\u001b[38;5;241m.\u001b[39mLoad():\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m ev_file\u001b[38;5;241m.\u001b[39mLoad():\n",
      "\u001b[1;31mTypeError\u001b[0m: DirectoryLoader.__init__() missing 1 required positional argument: 'loader_factory'"
     ]
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing import directory_loader\n",
    "from tensorboard.plugins.hparams import api_pb2\n",
    "\n",
    "run_dir = r\"../logs/sketch_transformer_experiment_0\"\n",
    "dl = directory_loader.DirectoryLoader(run_dir)\n",
    "for ev_file in dl.Load():\n",
    "    for e in ev_file.Load():\n",
    "        if not getattr(e, \"summary\", None):\n",
    "            continue\n",
    "        for v in e.summary.value:\n",
    "            md = getattr(v, \"metadata\", None)\n",
    "            if not (md and md.plugin_data and md.plugin_data.plugin_name == \"hparams\"):\n",
    "                continue\n",
    "            hp = api_pb2.HParamsPluginData()\n",
    "            hp.ParseFromString(md.plugin_data.content)\n",
    "            if hp.WhichOneof(\"data\") == \"session_start_info\":\n",
    "                ss = hp.session_start_info\n",
    "                def _to_py(val):\n",
    "                    kind = val.WhichOneof(\"kind\")\n",
    "                    return getattr(val, kind) if kind else None\n",
    "                hparams = {k: _to_py(v) for k, v in ss.hparams.items()}\n",
    "                print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82d23fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files: []\n",
      "records per file: []\n"
     ]
    }
   ],
   "source": [
    "import glob, tensorflow as tf, os\n",
    "files = glob.glob(os.path.join(logdir, \"events.out.tfevents.*\"))\n",
    "print(\"files:\", files)\n",
    "print(\"records per file:\", [sum(1 for _ in tf.data.TFRecordDataset([f])) for f in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "652a78cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment schema: None\n",
      "Found 0 trial(s). Example: []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import plugin_data_pb2  # message types\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "path = \"../logs/sketch_transformer_experiment_0/events.out.tfevents.1762733017.DESKTOP-6UC3P7J.22876.8\"\n",
    "\n",
    "experiment = None\n",
    "trials = []  # list of dicts: {\"run\": global_step or wall_time, \"hparams\": {...}}\n",
    "\n",
    "for e in tf.compat.v1.train.summary_iterator(path):\n",
    "    if not e.summary.value:\n",
    "        continue\n",
    "    for v in e.summary.value:\n",
    "        md = v.metadata\n",
    "        if md.plugin_data.plugin_name != \"hparams\":\n",
    "            continue\n",
    "\n",
    "        blob = md.plugin_data.content\n",
    "\n",
    "        # Try as Experiment config\n",
    "        exp = plugin_data_pb2.Experiment()\n",
    "        try:\n",
    "            exp.ParseFromString(blob)\n",
    "            experiment = MessageToDict(exp, preserving_proto_field_name=True)\n",
    "            continue\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Try as SessionStartInfo (actual hyperparameter values for a run)\n",
    "        ssi = plugin_data_pb2.SessionStartInfo()\n",
    "        try:\n",
    "            ssi.ParseFromString(blob)\n",
    "            ssi_dict = MessageToDict(ssi, preserving_proto_field_name=True)\n",
    "            # Convert list of {name, {string_value/float_value/bool_value}} to a plain dict\n",
    "            hp_dict = {}\n",
    "            for hp in ssi.hparams:\n",
    "                name = hp[\"name\"]\n",
    "                # pick whichever value field exists\n",
    "                val = hp.get(\"string_value\", hp.get(\"float_value\", hp.get(\"bool_value\")))\n",
    "                hp_dict[name] = val\n",
    "            trials.append({\"step\": e.step, \"wall_time\": e.wall_time, \"hparams\": hp_dict})\n",
    "            continue\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Try as SessionEndInfo (optional; not always needed)\n",
    "        sei = plugin_data_pb2.SessionEndInfo()\n",
    "        try:\n",
    "            sei.ParseFromString(blob)\n",
    "            # You could read status here if you care.\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "print(\"Experiment schema:\", experiment)\n",
    "print(\"Found\", len(trials), \"trial(s). Example:\", trials[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a2a16a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON-like hparams: []\n",
      "Scalar-like hparams: {}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tensorflow.python.framework import tensor_util\n",
    "\n",
    "path = \"../logs/sketch_transformer_experiment_0/1762733017.8895097/events.out.tfevents.1762733017.DESKTOP-6UC3P7J.22876.9\"\n",
    "hparam_json_hits = []\n",
    "scalar_hparams = {}\n",
    "\n",
    "for e in tf.compat.v1.train.summary_iterator(path):\n",
    "    for v in e.summary.value:\n",
    "        tag = v.tag or \"\"\n",
    "        # Text summary (JSON dumped as bytes in TensorProto)\n",
    "        if tag.lower().startswith(\"hparams\") and v.tensor.ByteSize() > 0:\n",
    "            arr = tensor_util.MakeNdarray(v.tensor)\n",
    "            if arr.size == 1 and isinstance(arr.item(), (bytes, str)):\n",
    "                txt = arr.item().decode() if isinstance(arr.item(), bytes) else arr.item()\n",
    "                try:\n",
    "                    hparam_json_hits.append(json.loads(txt))\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        # Scalar summaries masquerading as hparams\n",
    "        if tag.startswith(\"hparams/\") and v.HasField(\"simple_value\"):\n",
    "            name = tag.split(\"/\", 1)[1]\n",
    "            scalar_hparams[name] = v.simple_value\n",
    "\n",
    "print(\"JSON-like hparams:\", hparam_json_hits[:1])\n",
    "print(\"Scalar-like hparams:\", scalar_hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61678625",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7178651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, tensorflow as tf\n",
    "\n",
    "for path in glob.glob(\"/logs/sketch_transformer_experiment_0/**/events.out.tfevents.*\", recursive=True):\n",
    "    print(\"\\nInspecting:\", path)\n",
    "    try:\n",
    "        for e in tf.compat.v1.train.summary_iterator(path):\n",
    "            for v in e.summary.value:\n",
    "                print(\"  \", v.tag)\n",
    "            break  # just print first event\n",
    "    except Exception as ex:\n",
    "        print(\"  [error reading]\", ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf1e5ef1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "../logs/sketch_transformer_experiment_0/events.out.tfevents.1762733017.DESKTOP-6UC3P7J.22876.8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     57\u001b[0m     path_to \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../logs/sketch_transformer_experiment_0/events.out.tfevents.1762733017.DESKTOP-6UC3P7J.22876.8\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# your exact file\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mextract_hparams_from_event_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSession start @ step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstep\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "Cell \u001b[1;32mIn[39], line 23\u001b[0m, in \u001b[0;36mextract_hparams_from_event_file\u001b[1;34m(event_path)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Yield dicts of hparams found in the given events file.\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(event_path):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(event_path)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m summary_iterator(event_path):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m e\u001b[38;5;241m.\u001b[39msummary\u001b[38;5;241m.\u001b[39mvalue:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: ../logs/sketch_transformer_experiment_0/events.out.tfevents.1762733017.DESKTOP-6UC3P7J.22876.8"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# use the v1 iterator in TF2+\n",
    "summary_iterator = tf.compat.v1.train.summary_iterator\n",
    "\n",
    "# protobufs for the HParams plugin\n",
    "from tensorboard.plugins.hparams import api_pb2\n",
    "\n",
    "def _hp_value_to_py(v):\n",
    "    # api_pb2.HParamValue is a oneof: number_value / string_value / bool_value\n",
    "    if v.HasField(\"number_value\"):\n",
    "        return v.number_value\n",
    "    if v.HasField(\"string_value\"):\n",
    "        return v.string_value\n",
    "    if v.HasField(\"bool_value\"):\n",
    "        return v.bool_value\n",
    "    return None\n",
    "\n",
    "def extract_hparams_from_event_file(event_path):\n",
    "    \"\"\"Yield dicts of hparams found in the given events file.\"\"\"\n",
    "    if not os.path.exists(event_path):\n",
    "        raise FileNotFoundError(event_path)\n",
    "\n",
    "    for e in summary_iterator(event_path):\n",
    "        if not e.summary.value:\n",
    "            continue\n",
    "\n",
    "        for v in e.summary.value:\n",
    "            md = v.metadata\n",
    "            if not md.plugin_data.plugin_name:\n",
    "                continue\n",
    "\n",
    "            # HParams live under plugin_name == \"hparams\"\n",
    "            if md.plugin_data.plugin_name == \"hparams\":\n",
    "                pd = api_pb2.HParamsPluginData()\n",
    "                pd.ParseFromString(md.plugin_data.content)\n",
    "\n",
    "                # There are three oneof variants: experiment, session_start_info, session_end_info\n",
    "                if pd.HasField(\"session_start_info\"):\n",
    "                    ssi = pd.session_start_info\n",
    "                    hp = {k: _hp_value_to_py(v) for k, v in ssi.hparams.items()}\n",
    "                    yield {\n",
    "                        \"wall_time\": e.wall_time,\n",
    "                        \"step\": e.step,\n",
    "                        \"group_name\": ssi.group_name,      # may be empty\n",
    "                        \"session_name\": ssi.session_name,  # may be empty\n",
    "                        \"hparams\": hp,\n",
    "                    }\n",
    "\n",
    "                # You could also read experiment metadata or session_end_info if you want\n",
    "                # elif pd.HasField(\"experiment\"):\n",
    "                #     ...\n",
    "\n",
    "# ---- usage ----\n",
    "if __name__ == \"__main__\":\n",
    "    path_to = \"../logs/sketch_transformer_experiment_0/events.out.tfevents.1762733017.DESKTOP-6UC3P7J.22876.8\"  # your exact file\n",
    "    for record in extract_hparams_from_event_file(path_to):\n",
    "        print(\"Session start @ step\", record[\"step\"])\n",
    "        for k, v in record[\"hparams\"].items():\n",
    "            print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fa65f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "# Define some constants\n",
    "N_RUNS = 3\n",
    "N_EVENTS = 2\n",
    "# Prepare temp dirs for storing event files\n",
    "tmpdirs = {}\n",
    "\n",
    "tmpdirs['torch'] = tempfile.TemporaryDirectory()\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "log_dir = \"../logs2/logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a7d862a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sketch_transformer_example0/20251109_220241': {'d_model': [256.0, 256.0, 256.0], 'lr': [0.0001, 0.0001, 0.0001], 'max_len': [200.0, 200.0, 200.0], 'model': ['SketchTransformer', 'SketchTransformer', 'SketchTransformer'], 'n_layers': [6.0, 6.0, 6.0], 'nhead': [8.0, 8.0, 8.0], 'tokenizer': ['AbsolutePenPositionTokenizer', 'AbsolutePenPositionTokenizer', 'AbsolutePenPositionTokenizer'], 'tokenizer_bins': [32.0, 32.0, 32.0], 'use_padding_mask': [0.0, 0.0, 0.0]}, 'sketch_transformer_example0/20251109_221602': {'d_model': 384.0, 'lr': 0.0001, 'max_len': 200.0, 'model': 'SketchTransformer', 'n_layers': 6.0, 'nhead': 8.0, 'tokenizer': 'AbsolutePenPositionTokenizer', 'tokenizer_bins': 32.0, 'use_padding_mask': 0.0}, 'sketch_transformer_example0/20251109_223152': {'d_model': 256.0, 'lr': 0.0001, 'max_len': 200.0, 'model': 'SketchTransformer', 'n_layers': 8.0, 'nhead': 8.0, 'tokenizer': 'AbsolutePenPositionTokenizer', 'tokenizer_bins': 32.0, 'use_padding_mask': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "from tbparse import SummaryReader\n",
    "reader = SummaryReader(log_dir, pivot=True, extra_columns={'dir_name'})\n",
    "df = reader.hparams\n",
    "\n",
    "hparams_dict = df.set_index('dir_name').T.to_dict()\n",
    "print(hparams_dict)  # dictionary of hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aaf6008f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No scalar steps found in ../logs\\sketch_transformer_experiment_0\\1762737355.1614318\\events.out.tfevents.1762737355.DESKTOP-6UC3P7J.28764.9",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Example\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 44\u001b[0m     latest_step \u001b[38;5;241m=\u001b[39m \u001b[43mget_latest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLatest step:\u001b[39m\u001b[38;5;124m\"\u001b[39m, latest_step)\n",
      "Cell \u001b[1;32mIn[74], line 38\u001b[0m, in \u001b[0;36mget_latest_step\u001b[1;34m(log_dir)\u001b[0m\n\u001b[0;32m     35\u001b[0m         all_steps\u001b[38;5;241m.\u001b[39mappend(events[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstep)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_steps:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo scalar steps found in \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m latest_event_file)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(all_steps)\n",
      "\u001b[1;31mValueError\u001b[0m: No scalar steps found in ../logs\\sketch_transformer_experiment_0\\1762737355.1614318\\events.out.tfevents.1762737355.DESKTOP-6UC3P7J.28764.9"
     ]
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import os\n",
    "\n",
    "def get_latest_step(log_dir: str):\n",
    "    \"\"\"\n",
    "    Recursively search log_dir for TensorBoard event files, pick the most recently\n",
    "    modified one, and return the largest recorded step among all scalar tags.\n",
    "    \"\"\"\n",
    "    # Collect event files recursively\n",
    "    event_files = []\n",
    "    for root, _, files in os.walk(log_dir):\n",
    "        for f in files:\n",
    "            if f.startswith(\"events.out.tfevents\"):\n",
    "                event_files.append(os.path.join(root, f))\n",
    "\n",
    "    if not event_files:\n",
    "        raise FileNotFoundError(\"No TensorBoard event files found in \" + log_dir)\n",
    "\n",
    "    # Sort by modification time (latest last)\n",
    "    event_files.sort(key=lambda p: os.path.getmtime(p))\n",
    "    latest_event_file = event_files[-1]\n",
    "\n",
    "    # Load the event file\n",
    "    ea = event_accumulator.EventAccumulator(\n",
    "        latest_event_file,\n",
    "        size_guidance={\"scalars\": 0}  # load all scalars\n",
    "    )\n",
    "    ea.Reload()\n",
    "\n",
    "    # Gather all scalar tags and steps\n",
    "    all_steps = []\n",
    "    for tag in ea.Tags().get(\"scalars\", []):\n",
    "        events = ea.Scalars(tag)\n",
    "        if events:\n",
    "            all_steps.append(events[-1].step)\n",
    "\n",
    "    if not all_steps:\n",
    "        raise ValueError(\"No scalar steps found in \" + latest_event_file)\n",
    "\n",
    "    return max(all_steps)\n",
    "\n",
    "# Example\n",
    "if __name__ == \"__main__\":\n",
    "    latest_step = get_latest_step(\"../logs\")\n",
    "    print(\"Latest step:\", latest_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "eb863dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest step: 15\n"
     ]
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import os\n",
    "\n",
    "def get_latest_step(log_dir: str):\n",
    "    # Find the latest TensorBoard event file\n",
    "    event_files = [f for f in os.listdir(log_dir) if f.startswith(\"events.out.tfevents\")]\n",
    "    if not event_files:\n",
    "        raise FileNotFoundError(\"No TensorBoard event files found in \" + log_dir)\n",
    "    \n",
    "    # Sort by modification time (latest last)\n",
    "    event_files = sorted(event_files, key=lambda f: os.path.getmtime(os.path.join(log_dir, f)))\n",
    "    latest_event_file = os.path.join(log_dir, event_files[-1])\n",
    "\n",
    "    # Load the event file\n",
    "    ea = event_accumulator.EventAccumulator(latest_event_file)\n",
    "    ea.Reload()\n",
    "\n",
    "    # Gather all scalar tags and steps\n",
    "    all_steps = []\n",
    "    for tag in ea.Tags()['scalars']:\n",
    "        events = ea.Scalars(tag)\n",
    "        if events:\n",
    "            all_steps.append(events[-1].step)\n",
    "\n",
    "    if not all_steps:\n",
    "        raise ValueError(\"No scalar steps found in \" + log_dir)\n",
    "    \n",
    "    return max(all_steps)\n",
    "\n",
    "# Example\n",
    "latest_step = get_latest_step(\"../logs2/logs/sketch_transformer_example0/20251109_220241\")\n",
    "print(\"Latest step:\", latest_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "31ca61a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 64 scalar points. Example: [{'file': '../logs2/logs/sketch_transformer_example0/20251109_220241\\\\events.out.tfevents.1762743761.DESKTOP-99OM8G7.10288.0', 'tag': 'Loss/Train', 'step': 0, 'value': 2.116799831390381, 'wall_time': 1762743798.6510417}, {'file': '../logs2/logs/sketch_transformer_example0/20251109_220241\\\\events.out.tfevents.1762743761.DESKTOP-99OM8G7.10288.0', 'tag': 'Loss/Val', 'step': 0, 'value': 1.815959095954895, 'wall_time': 1762743799.9395761}, {'file': '../logs2/logs/sketch_transformer_example0/20251109_220241\\\\events.out.tfevents.1762743761.DESKTOP-99OM8G7.10288.0', 'tag': 'Accuracy/ValNextToken', 'step': 0, 'value': 0.13910172879695892, 'wall_time': 1762743799.9397447}]\n",
      "First 3 steps (wide rows):\n",
      "{'step': 0, 'wall_time': 1762743799.9504473, 'file': '../logs2/logs/sketch_transformer_example0/20251109_220241\\\\events.out.tfevents.1762743761.DESKTOP-99OM8G7.10288.0', 'Loss/Train': 2.116799831390381, 'Loss/Val': 1.815959095954895, 'Accuracy/ValNextToken': 0.13910172879695892, 'Perplexity/Val': 6.146969318389893}\n",
      "{'step': 1, 'wall_time': 1762743875.2242935, 'file': '../logs2/logs/sketch_transformer_example0/20251109_220241\\\\events.out.tfevents.1762743837.DESKTOP-99OM8G7.24680.0', 'Loss/Train': 1.6049363613128662, 'Loss/Val': 1.406111717224121, 'Accuracy/ValNextToken': 0.1575011909008026, 'Perplexity/Val': 4.080060005187988}\n",
      "{'step': 2, 'wall_time': 1762743911.5088537, 'file': '../logs2/logs/sketch_transformer_example0/20251109_220241\\\\events.out.tfevents.1762743837.DESKTOP-99OM8G7.24680.0', 'Loss/Train': 1.3870264291763306, 'Loss/Val': 1.300479769706726, 'Accuracy/ValNextToken': 0.17163392901420593, 'Perplexity/Val': 3.6710574626922607}\n",
      "{'step': 3, 'wall_time': 1762743948.1788042, 'file': '../logs2/logs/sketch_transformer_example0/20251109_220241\\\\events.out.tfevents.1762743837.DESKTOP-99OM8G7.24680.0', 'Loss/Train': 1.3151432275772095, 'Loss/Val': 1.2489317655563354, 'Accuracy/ValNextToken': 0.18236039578914642, 'Perplexity/Val': 3.486616611480713}\n",
      "{'step': 4, 'wall_time': 1762744016.848313, 'file': '../logs2/logs/sketch_transformer_example0/20251109_220241\\\\events.out.tfevents.1762743978.DESKTOP-99OM8G7.12672.0', 'Loss/Train': 1.275220513343811, 'Loss/Val': 1.2219630479812622, 'Accuracy/ValNextToken': 0.18730737268924713, 'Perplexity/Val': 3.393843650817871}\n",
      "{'step': 5, 'wall_time': 1762744053.0505285, 'file': '../logs2/logs/sketch_transformer_example0/20251109_220241\\\\events.out.tfevents.1762743978.DESKTOP-99OM8G7.12672.0', 'Loss/Train': 1.2484456300735474, 'Loss/Val': 1.1998791694641113, 'Accuracy/ValNextToken': 0.19173754751682281, 'Perplexity/Val': 3.319715738296509}\n",
      "{'step': 6, 'wall_time': 1762744089.2562675, 'file': '../logs2/logs/sketch_transformer_example0/20251109_220241\\\\events.out.tfevents.1762743978.DESKTOP-99OM8G7.12672.0', 'Loss/Train': 1.2288211584091187, 'Loss/Val': 1.182780385017395, 'Accuracy/ValNextToken': 0.1958601027727127, 'Perplexity/Val': 3.263435125350952}\n",
      "{'step': 7, 'wall_time': 1762744125.956463, 'file': '../logs2/logs/sketch_transformer_example0/20251109_220241\\\\events.out.tfevents.1762743978.DESKTOP-99OM8G7.12672.0', 'Loss/Train': 1.2152252197265625, 'Loss/Val': 1.1719419956207275, 'Accuracy/ValNextToken': 0.1982858031988144, 'Perplexity/Val': 3.2282557487487793}\n",
      "{'step': 8, 'wall_time': 1762744162.2916467, 'file': '../logs2/logs/sketch_transformer_example0/20251109_220241\\\\events.out.tfevents.1762743978.DESKTOP-99OM8G7.12672.0', 'Loss/Train': 1.2026878595352173, 'Loss/Val': 1.1613574028015137, 'Accuracy/ValNextToken': 0.20056188106536865, 'Perplexity/Val': 3.1942663192749023}\n",
      "{'step': 9, 'wall_time': 1762744200.225568, 'file': '../logs2/logs/sketch_transformer_example0/20251109_220241\\\\events.out.tfevents.1762743978.DESKTOP-99OM8G7.12672.0', 'Loss/Train': 1.192222237586975, 'Loss/Val': 1.1540923118591309, 'Accuracy/ValNextToken': 0.20292432606220245, 'Perplexity/Val': 3.1711437702178955}\n",
      "{'step': 10, 'wall_time': 1762744236.792859, 'file': '../logs2/logs/sketch_transformer_example0/20251109_220241\\\\events.out.tfevents.1762743978.DESKTOP-99OM8G7.12672.0', 'Loss/Train': 1.1831114292144775, 'Loss/Val': 1.1469612121582031, 'Accuracy/ValNextToken': 0.2043856382369995, 'Perplexity/Val': 3.1486103534698486}\n",
      "{'step': 11, 'wall_time': 1762744273.2710085, 'file': '../logs2/logs/sketch_transformer_example0/20251109_220241\\\\events.out.tfevents.1762743978.DESKTOP-99OM8G7.12672.0', 'Loss/Train': 1.175392985343933, 'Loss/Val': 1.1394912004470825, 'Accuracy/ValNextToken': 0.20643693208694458, 'Perplexity/Val': 3.1251778602600098}\n",
      "{'step': 12, 'wall_time': 1762744309.6094801, 'file': '../logs2/logs/sketch_transformer_example0/20251109_220241\\\\events.out.tfevents.1762743978.DESKTOP-99OM8G7.12672.0', 'Loss/Train': 1.1683074235916138, 'Loss/Val': 1.134104609489441, 'Accuracy/ValNextToken': 0.20759370923042297, 'Perplexity/Val': 3.108389139175415}\n",
      "{'step': 13, 'wall_time': 1762744345.6499183, 'file': '../logs2/logs/sketch_transformer_example0/20251109_220241\\\\events.out.tfevents.1762743978.DESKTOP-99OM8G7.12672.0', 'Loss/Train': 1.161873698234558, 'Loss/Val': 1.1302223205566406, 'Accuracy/ValNextToken': 0.20839938521385193, 'Perplexity/Val': 3.0963449478149414}\n",
      "{'step': 14, 'wall_time': 1762744381.967964, 'file': '../logs2/logs/sketch_transformer_example0/20251109_220241\\\\events.out.tfevents.1762743978.DESKTOP-99OM8G7.12672.0', 'Loss/Train': 1.155985713005066, 'Loss/Val': 1.1253420114517212, 'Accuracy/ValNextToken': 0.21033452451229095, 'Perplexity/Val': 3.081270456314087}\n",
      "Latest step: 15\n",
      "All scalars at latest step: {'Loss/Train': 1.1509594917297363, 'Loss/Val': 1.121171236038208, 'Accuracy/ValNextToken': 0.21162454783916473, 'Perplexity/Val': 3.068446159362793}\n"
     ]
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import os\n",
    "from typing import List, Dict, Any, Iterable, Tuple, Union\n",
    "\n",
    "def _event_files_under(path: str) -> List[str]:\n",
    "    if os.path.isfile(path):\n",
    "        return [path] if os.path.basename(path).startswith(\"events.out.tfevents\") else []\n",
    "    files = []\n",
    "    for root, _, fs in os.walk(path):\n",
    "        for f in fs:\n",
    "            if f.startswith(\"events.out.tfevents\"):\n",
    "                files.append(os.path.join(root, f))\n",
    "    return sorted(files, key=os.path.getmtime)\n",
    "\n",
    "def load_scalar_records(path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Return a LONG list of records: [{'file', 'tag', 'step', 'value', 'wall_time'} ...]\n",
    "    Each record is a scalar value logged at a specific step.\n",
    "    \"\"\"\n",
    "    records: List[Dict[str, Any]] = []\n",
    "    files = _event_files_under(path)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No TensorBoard event files found under: {path}\")\n",
    "\n",
    "    for evfile in files:\n",
    "        ea = event_accumulator.EventAccumulator(evfile)\n",
    "        ea.Reload()\n",
    "        for tag in ea.Tags().get('scalars', []):\n",
    "            for ev in ea.Scalars(tag):\n",
    "                records.append({\n",
    "                    \"file\": evfile,\n",
    "                    \"tag\": tag,\n",
    "                    \"step\": ev.step,\n",
    "                    \"value\": ev.value,\n",
    "                    \"wall_time\": ev.wall_time,\n",
    "                })\n",
    "    if not records:\n",
    "        raise ValueError(f\"No scalar values found in: {path}\")\n",
    "    return records\n",
    "\n",
    "def group_scalars_by_step(records: Iterable[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Collapse long records into one dict per step with all tags at that step.\n",
    "    If multiple files/runs have the same step+tag, the last (newest) by wall_time wins.\n",
    "    Returns a list sorted by step: [{'step', 'wall_time', 'file', '<tag1>': v1, ...}, ...]\n",
    "    \"\"\"\n",
    "    by_step: Dict[int, Dict[str, Any]] = {}\n",
    "    for r in records:\n",
    "        s = r[\"step\"]\n",
    "        bucket = by_step.setdefault(s, {\"step\": s, \"wall_time\": r[\"wall_time\"], \"file\": r[\"file\"]})\n",
    "        # Keep the newest wall_time/file if we see same step again\n",
    "        if r[\"wall_time\"] >= bucket[\"wall_time\"]:\n",
    "            bucket[\"wall_time\"] = r[\"wall_time\"]\n",
    "            bucket[\"file\"] = r[\"file\"]\n",
    "        bucket[r[\"tag\"]] = r[\"value\"]\n",
    "    return sorted(by_step.values(), key=lambda d: d[\"step\"])\n",
    "\n",
    "def latest_step_info(path: str) -> Tuple[int, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Convenience: return (latest_step, merged_row_for_that_step)\n",
    "    \"\"\"\n",
    "    records = load_scalar_records(path)\n",
    "    by_step = group_scalars_by_step(records)\n",
    "    row = by_step[-1]\n",
    "    return row[\"step\"], row\n",
    "\n",
    "# ---------- Usage ----------\n",
    "# 1) Get every scalar point (long format):\n",
    "records = load_scalar_records(\"../logs2/logs/sketch_transformer_example0/20251109_220241\")\n",
    "print(f\"Loaded {len(records)} scalar points. Example:\", records[:3])\n",
    "\n",
    "# 2) Group by step (wide format: one row per step, columns are tags):\n",
    "by_step = group_scalars_by_step(records)\n",
    "print(\"First 3 steps (wide rows):\")\n",
    "for row in by_step[:15]:\n",
    "    print(row)\n",
    "\n",
    "# 3) Latest step with all its tags:\n",
    "step, row = latest_step_info(\"../logs2/logs/sketch_transformer_example0/20251109_220241\")\n",
    "print(f\"Latest step: {step}\")\n",
    "print(\"All scalars at latest step:\", {k: v for k, v in row.items() if k not in ('step','wall_time','file')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2f6173a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorboard.plugins.hparams.api_pb2' has no attribute 'HParamValue'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 20\u001b[0m\n\u001b[0;32m     16\u001b[0m                 files\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, f))\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(files, key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetmtime)\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_hp_value_to_python\u001b[39m(pb_val: \u001b[43mapi_pb2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHParamValue\u001b[49m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# HParamValue oneof typically is 'string_value', 'number_value', or 'bool_value'\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     kind \u001b[38;5;241m=\u001b[39m pb_val\u001b[38;5;241m.\u001b[39mWhichOneof(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring_value\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorboard.plugins.hparams.api_pb2' has no attribute 'HParamValue'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "# These protos are part of TensorBoard's HParams plugin\n",
    "from tensorboard.plugins.hparams import api_pb2\n",
    "\n",
    "\n",
    "def _event_files_under(path: str) -> List[str]:\n",
    "    if os.path.isfile(path):\n",
    "        return [path] if os.path.basename(path).startswith(\"events.out.tfevents\") else []\n",
    "    files = []\n",
    "    for root, _, fs in os.walk(path):\n",
    "        for f in fs:\n",
    "            if f.startswith(\"events.out.tfevents\"):\n",
    "                files.append(os.path.join(root, f))\n",
    "    return sorted(files, key=os.path.getmtime)\n",
    "\n",
    "\n",
    "def _hp_value_to_python(pb_val: api_pb2.HParamValue) -> Any:\n",
    "    # HParamValue oneof typically is 'string_value', 'number_value', or 'bool_value'\n",
    "    kind = pb_val.WhichOneof(\"kind\")\n",
    "    if kind == \"string_value\":\n",
    "        return pb_val.string_value\n",
    "    if kind == \"number_value\":\n",
    "        # Cast to float or int if it is effectively integral\n",
    "        v = pb_val.number_value\n",
    "        return int(v) if float(v).is_integer() else float(v)\n",
    "    if kind == \"bool_value\":\n",
    "        return pb_val.bool_value\n",
    "    return None  # unknown kind\n",
    "\n",
    "\n",
    "def _try_parse_session_start(tensor_proto) -> api_pb2.SessionStartInfo | None:\n",
    "    \"\"\"\n",
    "    Attempt to parse a tensor's first string_val as a SessionStartInfo.\n",
    "    Returns the parsed proto or None on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not tensor_proto.string_val:\n",
    "            return None\n",
    "        raw = tensor_proto.string_val[0]\n",
    "        info = api_pb2.SessionStartInfo()\n",
    "        info.ParseFromString(raw)\n",
    "        # Heuristic: must have at least session_id or some hparams to be valid\n",
    "        if info.session_id or len(info.hparams) > 0:\n",
    "            return info\n",
    "    except Exception:\n",
    "        return None\n",
    "    return info\n",
    "\n",
    "\n",
    "def load_hparams_sessions(path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract HParams session starts from TensorBoard event files under `path`.\n",
    "\n",
    "    Returns a list like:\n",
    "    [\n",
    "      {\n",
    "        \"file\": \".../events.out.tfevents....\",\n",
    "        \"wall_time\": 1731192000.123,\n",
    "        \"session_id\": \"abc123\",\n",
    "        \"group_name\": \"exp_group\",\n",
    "        \"hparams\": {\"lr\": 0.001, \"batch_size\": 64, \"model\": \"transformer\", ...}\n",
    "      },\n",
    "      ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    files = _event_files_under(path)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No TensorBoard event files found under: {path}\")\n",
    "\n",
    "    sessions: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Size guidance: ensure tensors are loaded\n",
    "    size_guidance = {\n",
    "        event_accumulator.TENSORS: 0,\n",
    "        event_accumulator.SCALARS: 0,\n",
    "        event_accumulator.HISTOGRAMS: 0,\n",
    "        event_accumulator.IMAGES: 0,\n",
    "        event_accumulator.COMPRESSED_HISTOGRAMS: 0,\n",
    "    }\n",
    "\n",
    "    for evfile in files:\n",
    "        ea = event_accumulator.EventAccumulator(evfile, size_guidance=size_guidance)\n",
    "        ea.Reload()\n",
    "\n",
    "        tensor_tags = ea.Tags().get(\"tensors\", []) or []\n",
    "        # Prefer obvious names first, but we'll try all tags\n",
    "        preferred_order = [\"hparams\", \"hp/hparams\", \"hparams/session_start_info\"]\n",
    "        ordered_tags = sorted(tensor_tags, key=lambda t: (t not in preferred_order, t))\n",
    "\n",
    "        for tag in ordered_tags:\n",
    "            for t in ea.Tensors(tag):\n",
    "                info = _try_parse_session_start(t.tensor_proto)\n",
    "                if info is None:\n",
    "                    continue\n",
    "\n",
    "                # Convert map<string, HParamValue> to python dict\n",
    "                hp_dict = {k: _hp_value_to_python(v) for k, v in info.hparams.items()}\n",
    "\n",
    "                sessions.append({\n",
    "                    \"file\": evfile,\n",
    "                    \"wall_time\": t.wall_time,\n",
    "                    \"session_id\": info.session_id,     # may be empty if not set\n",
    "                    \"group_name\": info.group_name,     # may be empty if not set\n",
    "                    \"hparams\": hp_dict,\n",
    "                })\n",
    "\n",
    "    if not sessions:\n",
    "        raise ValueError(\n",
    "            \"No HParams (SessionStartInfo) found. \"\n",
    "            \"Make sure you logged hparams via TF HParams plugin or torch.add_hparams().\"\n",
    "        )\n",
    "\n",
    "    # Sort sessions by time, newest last\n",
    "    sessions.sort(key=lambda s: s[\"wall_time\"])\n",
    "    return sessions\n",
    "\n",
    "\n",
    "# ---------- Usage ----------\n",
    "# 1) Load all HParams sessions under a run directory or a single event file:\n",
    "hparam_sessions = load_hparams_sessions(\"../logs2/logs/sketch_transformer_example0/20251109_220241\")\n",
    "print(f\"Found {len(hparam_sessions)} HParams session(s). Example:\")\n",
    "print(hparam_sessions[-1])  # newest\n",
    "\n",
    "# 2) If you just want the \"latest\" hparams dict:\n",
    "latest_hparams = hparam_sessions[-1][\"hparams\"]\n",
    "print(\"Latest HParams:\", latest_hparams)\n",
    "\n",
    "# 3) If you want to merge multiple sessions (last value wins) into one dict:\n",
    "merged = {}\n",
    "for s in hparam_sessions:\n",
    "    merged.update(s[\"hparams\"])\n",
    "print(\"Merged HParams across sessions:\", merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13da33c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sebas\\Generative-SVG\\experiments\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: './logs2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mgetcwd())          \u001b[38;5;66;03m# shows your current working directory\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./logs2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# see what subfolders exist\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: './logs2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())          # shows your current working directory\n",
    "print(os.listdir(\".\"))  # see what subfolders exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705bb372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
