{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486bb518",
   "metadata": {},
   "source": [
    "# Test Experiments\n",
    "\n",
    "Frechet Inception Distance (FID) and Inception Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f69440ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_dir import set_cwd_project_root\n",
    "\n",
    "set_cwd_project_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9a864b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from runner import SketchTrainer, sample, device\n",
    "from prepare_data import add_svg_properties, clean_svg, stroke_to_bezier_single\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "import torchvision\n",
    "from raster_dataset import svg_rasterize\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "407a3fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_3ch_tensor(img_pil):\n",
    "    arr = np.array(img_pil, dtype=np.float32) / 255.0  # H×W in [0,1]\n",
    "    t = torch.from_numpy(arr).unsqueeze(0)  # 1×H×W\n",
    "    return t.repeat(3, 1, 1)  # 3×H×W\n",
    "\n",
    "\n",
    "def test_next_token_accuracy(sketch_trainer: SketchTrainer):\n",
    "    model = sketch_trainer.model\n",
    "    test_loader = sketch_trainer.test_loader\n",
    "    use_padding_mask = sketch_trainer.use_padding_mask\n",
    "    model.eval()\n",
    "    test_token_accuracy = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids, class_labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            class_labels = class_labels.to(device)\n",
    "\n",
    "            if use_padding_mask:\n",
    "                mask = input_ids == sketch_trainer.tokenizer.pad_token_id\n",
    "                logits = model(input_ids, class_labels, src_key_padding_mask=mask)\n",
    "            else:\n",
    "                logits = model(input_ids, class_labels)\n",
    "\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            mask = target_ids != sketch_trainer.tokenizer.pad_token_id\n",
    "            correct = (preds[mask] == target_ids[mask]).float().sum()\n",
    "            total = mask.sum()\n",
    "\n",
    "            acc = (correct / total) if total > 0 else torch.tensor(0.0, device=device)\n",
    "            test_token_accuracy += acc.item()\n",
    "\n",
    "    avg_acc = test_token_accuracy / len(test_loader)\n",
    "    print(f\"Test Next Token Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "\n",
    "def generate_autoregressive(\n",
    "    model,\n",
    "    class_labels,\n",
    "    tokenizer,\n",
    "    max_len,\n",
    "    device,\n",
    "    temperature=0.8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Autoregressively generate token sequences using SketchTransformer.\n",
    "\n",
    "    Returns:\n",
    "        seq: (batch, T) long tensor, starting with START, possibly ending with END.\n",
    "    \"\"\"\n",
    "    start_id = tokenizer.vocab[\"START\"]\n",
    "    end_id = tokenizer.vocab[\"END\"]\n",
    "\n",
    "    model.eval()\n",
    "    B = class_labels.size(0)\n",
    "    seq = torch.full(\n",
    "        (B, 1),\n",
    "        start_id,\n",
    "        dtype=torch.long,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    finished = torch.zeros(B, dtype=torch.bool, device=device)\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        logits = model(seq, class_labels)\n",
    "        next_logits = logits[:, -1, :]\n",
    "\n",
    "        # Only use the logits at the last time step\n",
    "        next_logits = logits[:, -1, :] / temperature  # (B, vocab_size)\n",
    "        probs = torch.softmax(next_logits, dim=-1)\n",
    "        next_tokens = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "\n",
    "        # Append next token\n",
    "        seq = torch.cat([seq, next_tokens.unsqueeze(1)], dim=1)\n",
    "\n",
    "        # Track which sequences have produced END\n",
    "        finished |= next_tokens == end_id\n",
    "        if finished.all():\n",
    "            break\n",
    "\n",
    "    return seq\n",
    "\n",
    "\n",
    "def trim_ids_for_decode(ids, end_id, pad_id):\n",
    "    \"\"\"\n",
    "    Trim a token list at the first END (inclusive) or PAD (exclusive), whichever comes first.\n",
    "    \"\"\"\n",
    "    cut = len(ids)\n",
    "    if end_id in ids:\n",
    "        cut = min(cut, ids.index(end_id) + 1)  # keep END\n",
    "    if pad_id in ids:\n",
    "        cut = min(cut, ids.index(pad_id))  # drop PAD\n",
    "    return ids[:cut]\n",
    "\n",
    "\n",
    "def test_fid_inception_score(\n",
    "    sketch_trainer: SketchTrainer,\n",
    "    num_samples=None,\n",
    "    bezier_postprocess=True,\n",
    "    canvas_size=128,\n",
    "):\n",
    "    model = sketch_trainer.model\n",
    "    test_loader = sketch_trainer.test_loader\n",
    "    tokenizer = sketch_trainer.tokenizer\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    start_id = tokenizer.vocab[\"START\"]\n",
    "    end_id = tokenizer.vocab[\"END\"]\n",
    "\n",
    "    def trim_ids_for_decode(ids):\n",
    "        cut = len(ids)\n",
    "        if end_id in ids:\n",
    "            cut = min(cut, ids.index(end_id) + 1)\n",
    "        if pad_id in ids:\n",
    "            cut = min(cut, ids.index(pad_id))\n",
    "        return ids[:cut]\n",
    "\n",
    "    # torchmetrics FID & IS\n",
    "    fid = FrechetInceptionDistance(normalize=False).to(device)\n",
    "    inception = InceptionScore(splits=10, normalize=False).to(device)\n",
    "\n",
    "    all_real_images = []  # will hold all images across every batch\n",
    "    all_fake_images = []\n",
    "\n",
    "    if num_samples is None:\n",
    "        num_samples = len(test_loader)\n",
    "\n",
    "    it = list(test_loader)[:num_samples]\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, target_ids, class_labels in tqdm(it, desc=\"FID/IS\"):\n",
    "            class_labels = class_labels.to(device)\n",
    "            max_len = model.max_len\n",
    "            fake_seqs = generate_autoregressive(\n",
    "                model=model,\n",
    "                class_labels=class_labels,\n",
    "                tokenizer=tokenizer,\n",
    "                max_len=max_len,\n",
    "                device=device,\n",
    "            )\n",
    "            fake_cpu = fake_seqs.cpu()\n",
    "\n",
    "            real_batch = []\n",
    "            fake_batch = []\n",
    "\n",
    "            B = fake_cpu.size(0)\n",
    "            for b in range(B):\n",
    "                real_ids = [start_id] + [\n",
    "                    t for t in target_ids[b].tolist() if t != pad_id\n",
    "                ]\n",
    "                real_ids = trim_ids_for_decode(real_ids)\n",
    "                real_svg = tokenizer.decode(real_ids)\n",
    "                if bezier_postprocess:\n",
    "                    real_svg = stroke_to_bezier_single(real_svg)\n",
    "                    real_svg = clean_svg(real_svg)\n",
    "                real_svg = add_svg_properties(\n",
    "                    real_svg, width=canvas_size, height=canvas_size\n",
    "                )\n",
    "                real_img = svg_rasterize(real_svg)  # PIL grayscale image\n",
    "                r = to_3ch_tensor(real_img)\n",
    "\n",
    "                fake_ids = [t for t in fake_cpu[b].tolist() if t != pad_id]\n",
    "                fake_ids = trim_ids_for_decode(fake_ids)\n",
    "                fake_svg = tokenizer.decode(fake_ids)\n",
    "                if bezier_postprocess:\n",
    "                    fake_svg = stroke_to_bezier_single(fake_svg)\n",
    "                    fake_svg = clean_svg(fake_svg)\n",
    "                fake_svg = add_svg_properties(\n",
    "                    fake_svg, width=canvas_size, height=canvas_size\n",
    "                )\n",
    "\n",
    "                fake_img = svg_rasterize(fake_svg)\n",
    "                f = to_3ch_tensor(fake_img)\n",
    "\n",
    "                real_batch.append(r.unsqueeze(0))  # 1×3×H×W\n",
    "                fake_batch.append(f.unsqueeze(0))\n",
    "\n",
    "            all_real_images.extend(real_batch)\n",
    "            all_fake_images.extend(fake_batch)\n",
    "\n",
    "            real_images = torch.cat(real_batch, dim=0).to(\n",
    "                device=device, dtype=torch.uint8\n",
    "            )\n",
    "            fake_images = torch.cat(fake_batch, dim=0).to(\n",
    "                device=device, dtype=torch.uint8\n",
    "            )\n",
    "\n",
    "            fid.update(real_images, real=True)\n",
    "            fid.update(fake_images, real=False)\n",
    "            inception.update(fake_images)\n",
    "\n",
    "    all_real_images = torch.cat(all_real_images, dim=0).cpu()\n",
    "    all_fake_images = torch.cat(all_fake_images, dim=0).cpu()\n",
    "\n",
    "    # save one image each\n",
    "    torchvision.utils.save_image(all_real_images, \"real_grid.png\")\n",
    "    torchvision.utils.save_image(all_fake_images, \"fake_grid.png\")\n",
    "\n",
    "    fid_score = fid.compute().item()\n",
    "    is_mean, is_std = inception.compute()\n",
    "    is_mean = is_mean.item()\n",
    "    is_std = is_std.item()\n",
    "\n",
    "    sketch_trainer.writer.add_scalar(\"FID/Test\", fid_score, 0)\n",
    "    sketch_trainer.writer.add_scalar(\"IS/TestMean\", is_mean, 0)\n",
    "    sketch_trainer.writer.add_scalar(\"IS/TestStd\", is_std, 0)\n",
    "\n",
    "    print(f\"Test FID: {fid_score:.4f}\")\n",
    "    print(f\"Test Inception Score: mean={is_mean:.4f}, std={is_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a30daa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "def test_clip_score(\n",
    "    sketch_trainer: SketchTrainer,\n",
    "    dataset,\n",
    "    num_samples=None,\n",
    "    canvas_size=512,\n",
    "    bezier_postprocess=True,\n",
    "):\n",
    "    model = sketch_trainer.model\n",
    "    test_loader = sketch_trainer.test_loader\n",
    "    tokenizer = sketch_trainer.tokenizer\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    start_id = tokenizer.vocab[\"START\"]\n",
    "    end_id = tokenizer.vocab[\"END\"]\n",
    "\n",
    "    def trim_ids_for_decode(ids):\n",
    "        cut = len(ids)\n",
    "        if end_id in ids:\n",
    "            cut = min(cut, ids.index(end_id) + 1)\n",
    "        if pad_id in ids:\n",
    "            cut = min(cut, ids.index(pad_id))\n",
    "        return ids[:cut]\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Load CLIP\n",
    "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "    real_scores = []\n",
    "    fake_scores = []\n",
    "\n",
    "    # num_samples = number of batches to use\n",
    "    if num_samples is None:\n",
    "        num_samples = len(test_loader)\n",
    "\n",
    "    it = list(test_loader)[:num_samples]\n",
    "\n",
    "    model.eval()\n",
    "    clip_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, target_ids, class_labels in tqdm(it, desc=\"CLIP Score\"):\n",
    "            class_labels = class_labels.to(device)\n",
    "            max_len = model.max_len\n",
    "\n",
    "            fake_seqs = generate_autoregressive(\n",
    "                model=model,\n",
    "                class_labels=class_labels,\n",
    "                tokenizer=tokenizer,\n",
    "                max_len=max_len,\n",
    "                device=device,\n",
    "            )\n",
    "            fake_cpu = fake_seqs.cpu()\n",
    "\n",
    "            # Convert numeric labels → strings\n",
    "            text_labels = [dataset.label_map[c.item()] for c in class_labels]\n",
    "            text = [f\"a {lbl} sketch\" for lbl in text_labels]\n",
    "\n",
    "            # Encode text using CLIP\n",
    "            text_inputs = processor(text=text, return_tensors=\"pt\", padding=True).to(device)\n",
    "            text_embeds = clip_model.get_text_features(**text_inputs)\n",
    "            text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            B = fake_cpu.size(0)\n",
    "\n",
    "            for b in range(B):\n",
    "                # --- REAL ---\n",
    "                real_ids = [start_id] + [t for t in target_ids[b].tolist() if t != pad_id]\n",
    "                real_ids = trim_ids_for_decode(real_ids)\n",
    "                real_svg = tokenizer.decode(real_ids)\n",
    "\n",
    "                if bezier_postprocess:\n",
    "                    real_svg = stroke_to_bezier_single(real_svg)\n",
    "                    real_svg = clean_svg(real_svg)\n",
    "\n",
    "                real_svg = add_svg_properties(real_svg, width=canvas_size, height=canvas_size)\n",
    "                real_img = svg_rasterize(real_svg).convert(\"RGB\")\n",
    "\n",
    "                # --- FAKE ---\n",
    "                fake_ids = [t for t in fake_cpu[b].tolist() if t != pad_id]\n",
    "                fake_ids = trim_ids_for_decode(fake_ids)\n",
    "                fake_svg = tokenizer.decode(fake_ids)\n",
    "\n",
    "                if bezier_postprocess:\n",
    "                    fake_svg = stroke_to_bezier_single(fake_svg)\n",
    "                    fake_svg = clean_svg(fake_svg)\n",
    "\n",
    "                fake_svg = add_svg_properties(fake_svg, width=canvas_size, height=canvas_size)\n",
    "                fake_img = svg_rasterize(fake_svg).convert(\"RGB\")\n",
    "\n",
    "                # Encode both images at once\n",
    "                image_inputs = processor(images=[real_img, fake_img], return_tensors=\"pt\").to(device)\n",
    "                image_embeds = clip_model.get_image_features(**image_inputs)\n",
    "                image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
    "\n",
    "                # Compute cosine similarity\n",
    "                # image_embeds: (2, D)\n",
    "                # text_embeds: (B, D)\n",
    "                # We want the score of this sample only => index b\n",
    "                scores = (image_embeds @ text_embeds[b].unsqueeze(1)).squeeze().tolist()\n",
    "\n",
    "                real_clip_score = scores[0]\n",
    "                fake_clip_score = scores[1]\n",
    "\n",
    "                real_scores.append(real_clip_score)\n",
    "                fake_scores.append(fake_clip_score)\n",
    "\n",
    "    avg_real_score = sum(real_scores) / len(real_scores)\n",
    "    avg_fake_score = sum(fake_scores) / len(fake_scores)\n",
    "\n",
    "    print(f\"Average Real CLIP Score: {avg_real_score:.4f}\")\n",
    "    print(f\"Average Fake CLIP Score: {avg_fake_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41e2adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade pip\n",
    "# %pip install --upgrade Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae56c465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading QuickDrawDataset files: 100%|██████████| 5/5 [00:00<00:00, 11416.18it/s]\n",
      "Loading QuickDrawDataset: 5it [00:00, 338.58it/s]\n",
      "Tokenizing dataset: 100%|██████████| 5/5 [00:00<00:00, 629.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumed training from checkpoint: C:\\Code\\Generative-SVG\\_site\\model_checkpoint_15.pt\n"
     ]
    }
   ],
   "source": [
    "from dataset import QuickDrawDataset\n",
    "from sketch_tokenizers import DeltaPenPositionTokenizer\n",
    "from models import SketchTransformerConditional\n",
    "from runner import SketchTrainer, sample\n",
    "\n",
    "label_names = [\"cake\" ,\"butterfly\",\"flower\",\"mug\",\"sea turtle\"]\n",
    "dataset = QuickDrawDataset(label_names=label_names, download=True)\n",
    "tokenizer = DeltaPenPositionTokenizer(bins=32)\n",
    "\n",
    "model = SketchTransformerConditional(\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_layers=8,\n",
    "    max_len=200,\n",
    "    num_classes=len(label_names),\n",
    ")\n",
    "\n",
    "training_config = {\n",
    "    \"batch_size\": 128,\n",
    "    \"num_epochs\": 15,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"log_dir\": \"logs/tes1\",\n",
    "    \"splits\": [0.85, 0.075, 0.075],\n",
    "    # \"use_padding_mask\": True,\n",
    "    \"checkpoint_path\": \"_site/model_checkpoint_15.pt\",\n",
    "}\n",
    "\n",
    "trainer = SketchTrainer(model, dataset, tokenizer, training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6eff09c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_next_token_accuracy(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66d1e85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "FID/IS: 100%|██████████| 20/20 [03:28<00:00, 10.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test FID: 1.3663\n",
      "Test Inception Score: mean=1.0852, std=0.0064\n"
     ]
    }
   ],
   "source": [
    "test_fid_inception_score(trainer, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dce68779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID/IS: 100%|██████████| 20/20 [03:55<00:00, 11.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test FID: 1.4441\n",
      "Test Inception Score: mean=1.0668, std=0.0034\n"
     ]
    }
   ],
   "source": [
    "test_fid_inception_score(trainer, 20, bezier_postprocess=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e1d0734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID/IS: 100%|██████████| 20/20 [03:31<00:00, 10.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test FID: 0.3535\n",
      "Test Inception Score: mean=1.0294, std=0.0010\n"
     ]
    }
   ],
   "source": [
    "test_fid_inception_score(trainer, 20, canvas_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d19c1ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "CLIP Score: 100%|██████████| 20/20 [05:13<00:00, 15.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Real CLIP Score: 0.2429\n",
      "Average Fake CLIP Score: 0.2406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_clip_score(trainer, dataset, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
