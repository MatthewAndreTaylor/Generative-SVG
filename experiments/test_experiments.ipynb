{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486bb518",
   "metadata": {},
   "source": [
    "# Test Experiments\n",
    "\n",
    "Frechet Inception Distance (FID) and Inception Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f69440ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_dir import set_cwd_project_root\n",
    "\n",
    "set_cwd_project_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "407a3fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from runner import SketchTrainer, device\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from raster_dataset import svg_rasterize\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def to_3ch_tensor(img_pil):\n",
    "    arr = np.array(img_pil, dtype=np.uint8)          # H×W (grayscale)\n",
    "    t = torch.from_numpy(arr).unsqueeze(0)           # 1×H×W\n",
    "    return t.repeat(3, 1, 1)                         # 3×H×W\n",
    "\n",
    "\n",
    "def test_it(sketch_trainer: SketchTrainer):\n",
    "    model = sketch_trainer.model\n",
    "    test_loader = sketch_trainer.test_loader\n",
    "    use_padding_mask = sketch_trainer.use_padding_mask\n",
    "\n",
    "    model.eval()\n",
    "    test_token_accuracy = 0.0\n",
    "\n",
    "    # ---------- 1) Next-token accuracy ----------\n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids, class_labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            class_labels = class_labels.to(device)\n",
    "\n",
    "            if use_padding_mask:\n",
    "                mask = input_ids == sketch_trainer.tokenizer.pad_token_id\n",
    "                logits = model(input_ids, class_labels, src_key_padding_mask=mask)\n",
    "            else:\n",
    "                logits = model(input_ids, class_labels)\n",
    "\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            mask = target_ids != sketch_trainer.tokenizer.pad_token_id\n",
    "            correct = (preds[mask] == target_ids[mask]).float().sum()\n",
    "            total = mask.sum()\n",
    "\n",
    "            acc = (correct / total) if total > 0 else torch.tensor(0.0, device=device)\n",
    "            test_token_accuracy += acc.item()\n",
    "\n",
    "    avg_acc = test_token_accuracy / len(test_loader)\n",
    "    print(f\"Test Next Token Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "    # ---------- 2) FID + IS ----------\n",
    "    start_id = sketch_trainer.tokenizer.vocab[\"START\"]\n",
    "    end_id = sketch_trainer.tokenizer.vocab[\"END\"]\n",
    "\n",
    "    def _trim_at_end(ids):\n",
    "        if end_id in ids:\n",
    "            idx = ids.index(end_id)\n",
    "            return ids[: idx + 1]\n",
    "        return ids\n",
    "\n",
    "    # torchmetrics FID: expects 3×HxW uint8 or float with [0,1] if normalize=True\n",
    "    fid = FrechetInceptionDistance(normalize=False).to(device)\n",
    "    inception = InceptionScore(splits=10, normalize=False).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids, class_labels in tqdm(test_loader, desc=\"FID/IS\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            class_labels = class_labels.to(device)\n",
    "\n",
    "            if use_padding_mask:\n",
    "                mask = input_ids == sketch_trainer.tokenizer.pad_token_id\n",
    "                logits = model(input_ids, class_labels, src_key_padding_mask=mask)\n",
    "            else:\n",
    "                logits = model(input_ids, class_labels)\n",
    "\n",
    "            preds = logits.argmax(dim=-1).cpu()\n",
    "            targets_cpu = target_ids.cpu()\n",
    "\n",
    "            real_batch = []\n",
    "            fake_batch = []\n",
    "\n",
    "            B = preds.size(0)\n",
    "            for b in range(B):\n",
    "                # REAL sequence\n",
    "                real_ids = [start_id] + targets_cpu[b].tolist()\n",
    "                real_ids = _trim_at_end(real_ids)\n",
    "                real_svg = sketch_trainer.tokenizer.decode(real_ids)\n",
    "                real_img = svg_rasterize(real_svg)     # PIL, grayscale 299×299\n",
    "                r = to_3ch_tensor(real_img)            # 3×299×299\n",
    "\n",
    "                # FAKE sequence\n",
    "                fake_ids = [start_id] + preds[b].tolist()\n",
    "                fake_ids = _trim_at_end(fake_ids)\n",
    "                fake_svg = sketch_trainer.tokenizer.decode(fake_ids)\n",
    "                fake_img = svg_rasterize(fake_svg)\n",
    "                f = to_3ch_tensor(fake_img)\n",
    "\n",
    "                real_batch.append(r.unsqueeze(0))      # 1×3×H×W\n",
    "                fake_batch.append(f.unsqueeze(0))\n",
    "\n",
    "            real_images = torch.cat(real_batch, dim=0).to(device)  # B×3×H×W\n",
    "            fake_images = torch.cat(fake_batch, dim=0).to(device)\n",
    "\n",
    "            fid.update(real_images, real=True)\n",
    "            fid.update(fake_images, real=False)\n",
    "            inception.update(fake_images)\n",
    "\n",
    "    fid_score = fid.compute().item()\n",
    "    is_mean, is_std = inception.compute()\n",
    "    is_mean = is_mean.item()\n",
    "    is_std = is_std.item()\n",
    "\n",
    "    sketch_trainer.writer.add_scalar(\"FID/Test\", fid_score, 0)\n",
    "    sketch_trainer.writer.add_scalar(\"IS/TestMean\", is_mean, 0)\n",
    "    sketch_trainer.writer.add_scalar(\"IS/TestStd\", is_std, 0)\n",
    "\n",
    "    print(f\"Test FID: {fid_score:.4f}\")\n",
    "    print(f\"Test Inception Score: mean={is_mean:.4f}, std={is_std:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef96c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading QuickDrawDataset files: 100%|██████████| 1/1 [00:00<00:00, 1262.96it/s]\n",
      "Loading QuickDrawDataset: 1it [00:00, 1047.27it/s]\n",
      "Tokenizing dataset: 100%|██████████| 1/1 [00:00<00:00, 2928.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, starting fresh training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initial Eval: 100%|██████████| 81/81 [00:00<00:00, 246.72it/s]\n",
      "Testing: 100%|██████████| 41/41 [00:01<00:00, 24.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Next Token Accuracy: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID/IS: 100%|██████████| 41/41 [00:27<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test FID: 220.4308\n",
      "Test Inception Score: mean=1.9268, std=0.0229\n"
     ]
    }
   ],
   "source": [
    "trainer = load_config(\"configs/example_0.toml\")\n",
    "test_it(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eab12148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from runner import SketchTrainer, device\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from raster_dataset import svg_rasterize\n",
    "import numpy as np\n",
    "\n",
    "def to_3ch_tensor(img_pil):\n",
    "    arr = np.array(img_pil, dtype=np.float32) / 255.0  # H×W in [0,1]\n",
    "    t = torch.from_numpy(arr).unsqueeze(0)              # 1×H×W\n",
    "    return t.repeat(3, 1, 1)                            # 3×H×W\n",
    "\n",
    "def generate_autoregressive(\n",
    "    model,\n",
    "    class_labels,\n",
    "    tokenizer,\n",
    "    max_len,\n",
    "    use_padding_mask: bool,\n",
    "    device,\n",
    "):\n",
    "    \"\"\"\n",
    "    Autoregressively generate token sequences using SketchTransformer.\n",
    "\n",
    "    Returns:\n",
    "        seq: (batch, T) long tensor, starting with START, possibly ending with END.\n",
    "    \"\"\"\n",
    "    pad_id   = tokenizer.pad_token_id\n",
    "    start_id = tokenizer.vocab[\"START\"]\n",
    "    end_id   = tokenizer.vocab[\"END\"]\n",
    "\n",
    "    model.eval()\n",
    "    B = class_labels.size(0)\n",
    "\n",
    "    # Start with START only\n",
    "    seq = torch.full(\n",
    "        (B, 1),\n",
    "        start_id,\n",
    "        dtype=torch.long,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    finished = torch.zeros(B, dtype=torch.bool, device=device)\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        if use_padding_mask:\n",
    "            pad_mask = seq == pad_id  # (B, T)\n",
    "            logits = model(seq, class_labels, src_key_padding_mask=pad_mask)\n",
    "        else:\n",
    "            logits = model(seq, class_labels)\n",
    "\n",
    "        # Only use the logits at the last time step\n",
    "        next_logits = logits[:, -1, :]   # (B, vocab_size)\n",
    "        next_tokens = next_logits.argmax(dim=-1)  # (B,)\n",
    "\n",
    "        # Append next token\n",
    "        seq = torch.cat([seq, next_tokens.unsqueeze(1)], dim=1)\n",
    "\n",
    "        # Track which sequences have produced END\n",
    "        finished |= (next_tokens == end_id)\n",
    "        if finished.all():\n",
    "            break\n",
    "\n",
    "    return seq\n",
    "\n",
    "def trim_ids_for_decode(ids, end_id, pad_id):\n",
    "    \"\"\"\n",
    "    Trim a token list at the first END (inclusive) or PAD (exclusive), whichever comes first.\n",
    "    \"\"\"\n",
    "    cut = len(ids)\n",
    "    if end_id in ids:\n",
    "        cut = min(cut, ids.index(end_id) + 1)  # keep END\n",
    "    if pad_id in ids:\n",
    "        cut = min(cut, ids.index(pad_id))      # drop PAD\n",
    "    return ids[:cut]\n",
    "\n",
    "def test_it(sketch_trainer: SketchTrainer):\n",
    "    model = sketch_trainer.model\n",
    "    test_loader = sketch_trainer.test_loader\n",
    "    use_padding_mask = sketch_trainer.use_padding_mask\n",
    "\n",
    "    model.eval()\n",
    "    test_token_accuracy = 0.0\n",
    "\n",
    "    # ---------- 1) Next-token accuracy (unchanged) ----------\n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids, class_labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            class_labels = class_labels.to(device)\n",
    "\n",
    "            if use_padding_mask:\n",
    "                mask = input_ids == sketch_trainer.tokenizer.pad_token_id\n",
    "                logits = model(input_ids, class_labels, src_key_padding_mask=mask)\n",
    "            else:\n",
    "                logits = model(input_ids, class_labels)\n",
    "\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            mask = target_ids != sketch_trainer.tokenizer.pad_token_id\n",
    "            correct = (preds[mask] == target_ids[mask]).float().sum()\n",
    "            total = mask.sum()\n",
    "\n",
    "            acc = (correct / total) if total > 0 else torch.tensor(0.0, device=device)\n",
    "            test_token_accuracy += acc.item()\n",
    "\n",
    "    avg_acc = test_token_accuracy / len(test_loader)\n",
    "    print(f\"Test Next Token Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "    # ---------- 2) FID + IS with proper generation ----------\n",
    "    tokenizer = sketch_trainer.tokenizer\n",
    "    pad_id   = tokenizer.pad_token_id\n",
    "    start_id = tokenizer.vocab[\"START\"]\n",
    "    end_id   = tokenizer.vocab[\"END\"]\n",
    "\n",
    "    def trim_ids_for_decode(ids):\n",
    "        cut = len(ids)\n",
    "        if end_id in ids:\n",
    "            cut = min(cut, ids.index(end_id) + 1)\n",
    "        if pad_id in ids:\n",
    "            cut = min(cut, ids.index(pad_id))\n",
    "        return ids[:cut]\n",
    "\n",
    "    # torchmetrics FID & IS\n",
    "    fid = FrechetInceptionDistance(normalize=False).to(device)\n",
    "    inception = InceptionScore(splits=10, normalize=False).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, target_ids, class_labels in tqdm(test_loader, desc=\"FID/IS\"):\n",
    "            target_ids = target_ids.to(device)\n",
    "            class_labels = class_labels.to(device)\n",
    "\n",
    "            # 1) REAL sequences: from dataset targets\n",
    "            targets_cpu = target_ids.cpu()\n",
    "\n",
    "            # 2) FAKE sequences: from *autoregressive* generation\n",
    "            max_len = getattr(sketch_trainer, \"max_len\", model.max_len)\n",
    "            fake_seqs = generate_autoregressive(\n",
    "                model=model,\n",
    "                class_labels=class_labels,\n",
    "                tokenizer=tokenizer,\n",
    "                max_len=max_len,\n",
    "                use_padding_mask=use_padding_mask,\n",
    "                device=device,\n",
    "            )\n",
    "            fake_cpu = fake_seqs.cpu()\n",
    "\n",
    "            real_batch = []\n",
    "            fake_batch = []\n",
    "\n",
    "            B = fake_cpu.size(0)\n",
    "            for b in range(B):\n",
    "                # ----- REAL -----\n",
    "                # If your targets already include START, don't add another START here.\n",
    "                # This version assumes targets DO NOT include START.\n",
    "                real_ids = [start_id] + [\n",
    "                    t for t in targets_cpu[b].tolist() if t != pad_id\n",
    "                ]\n",
    "                real_ids = trim_ids_for_decode(real_ids)\n",
    "                real_svg = tokenizer.decode(real_ids)\n",
    "                real_img = svg_rasterize(real_svg)   # PIL grayscale 299×299 (or similar)\n",
    "                r = to_3ch_tensor(real_img)\n",
    "\n",
    "                # ----- FAKE -----\n",
    "                fake_ids = [t for t in fake_cpu[b].tolist() if t != pad_id]\n",
    "                fake_ids = trim_ids_for_decode(fake_ids)\n",
    "                fake_svg = tokenizer.decode(fake_ids)\n",
    "                fake_img = svg_rasterize(fake_svg)\n",
    "                f = to_3ch_tensor(fake_img)\n",
    "\n",
    "                real_batch.append(r.unsqueeze(0))  # 1×3×H×W\n",
    "                fake_batch.append(f.unsqueeze(0))\n",
    "\n",
    "            real_images = torch.cat(real_batch, dim=0).to(device=device, dtype=torch.uint8)\n",
    "            fake_images = torch.cat(fake_batch, dim=0).to(device=device, dtype=torch.uint8)\n",
    "\n",
    "            fid.update(real_images, real=True)\n",
    "            fid.update(fake_images, real=False)\n",
    "            inception.update(fake_images)\n",
    "\n",
    "    fid_score = fid.compute().item()\n",
    "    is_mean, is_std = inception.compute()\n",
    "    is_mean = is_mean.item()\n",
    "    is_std = is_std.item()\n",
    "\n",
    "    sketch_trainer.writer.add_scalar(\"FID/Test\", fid_score, 0)\n",
    "    sketch_trainer.writer.add_scalar(\"IS/TestMean\", is_mean, 0)\n",
    "    sketch_trainer.writer.add_scalar(\"IS/TestStd\", is_std, 0)\n",
    "\n",
    "    print(f\"Test FID: {fid_score:.4f}\")\n",
    "    print(f\"Test Inception Score: mean={is_mean:.4f}, std={is_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3308283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading QuickDrawDataset files: 100%|██████████| 1/1 [00:00<00:00, 1099.71it/s]\n",
      "Loading QuickDrawDataset: 1it [00:00, 963.32it/s]\n",
      "Tokenizing dataset: 100%|██████████| 1/1 [00:00<00:00, 3111.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, starting fresh training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initial Eval: 100%|██████████| 81/81 [00:00<00:00, 438.76it/s]\n",
      "Testing: 100%|██████████| 41/41 [00:01<00:00, 24.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Next Token Accuracy: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID/IS: 100%|██████████| 41/41 [02:46<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test FID: 10.1799\n",
      "Test Inception Score: mean=1.0000, std=0.0000\n"
     ]
    }
   ],
   "source": [
    "from main import load_config, SketchTrainer\n",
    "\n",
    "trainer = load_config(\"configs/example_0.toml\")\n",
    "test_it(trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
