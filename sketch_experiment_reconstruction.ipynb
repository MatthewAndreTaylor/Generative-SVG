{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632aa9ac",
   "metadata": {},
   "source": [
    "# Initial experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "651fca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset import QuickDrawDataset\n",
    "from utils import AbsolutePenPositionTokenizer\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0c0017c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d0dd2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading QuickDraw files: 100%|██████████| 1/1 [00:00<00:00, 11428.62it/s]\n",
      "Loading QuickDraw files: 100%|██████████| 1/1 [00:01<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized data from sketch-im_tokenized_dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "import pyvips\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "def svg_rasterize(svg_string: str, size=(32, 32)) -> Image.Image:\n",
    "    svg_bytes = svg_string.encode('utf-8')\n",
    "    image = pyvips.Image.svgload_buffer(svg_bytes)\n",
    "    image = image.flatten(background=0xFFFFFF)\n",
    "    png_bytes = image.write_to_buffer(\".png\")\n",
    "    img = Image.open(BytesIO(png_bytes)).convert(\"L\")\n",
    "    img = img.resize(size)\n",
    "    tensor = torch.tensor(list(img.getdata()), dtype=torch.float32)\n",
    "    tensor = tensor.view(size[1], size[0]) / 255.0\n",
    "    return tensor\n",
    "\n",
    "labels = [\"cat\"]\n",
    "\n",
    "training_data = QuickDrawDataset(\n",
    "    labels=labels,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "tokenizer = AbsolutePenPositionTokenizer(bins=64)\n",
    "\n",
    "class SketchReconDataset(Dataset):\n",
    "    def __init__(self, svg_list, tokenizer, max_len=200, cache_file=\"sketch-im_tokenized_dataset.pkl\"):\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.pad_id = tokenizer.vocab[\"PAD\"]\n",
    "        \n",
    "        # Try to load from cache\n",
    "        try:\n",
    "            with open(cache_file, \"rb\") as f:\n",
    "                self.data = pickle.load(f)\n",
    "            print(f\"Loaded tokenized data from {cache_file}\")\n",
    "        except FileNotFoundError:\n",
    "            for svg in tqdm(svg_list, desc=\"Tokenizing SVGs\"):\n",
    "                tokens = tokenizer.encode(svg)\n",
    "                # Truncate + Pad\n",
    "                tokens = tokens[:max_len]\n",
    "                tokens = tokens + [self.pad_id] * (max_len - len(tokens))\n",
    "                image = svg_rasterize(svg)\n",
    "                self.data.append((tokens, image))\n",
    "\n",
    "            # Save to cache\n",
    "            with open(cache_file, \"wb\") as f:\n",
    "                pickle.dump(self.data, f)\n",
    "            print(f\"Saved tokenized data to {cache_file}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq, image = self.data[idx]\n",
    "        input_ids = torch.tensor(seq[:-1])\n",
    "        target_ids = torch.tensor(seq[1:])\n",
    "        return image, input_ids, target_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "dataset = SketchReconDataset(training_data, tokenizer, max_len=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
