{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632aa9ac",
   "metadata": {},
   "source": [
    "# Initial experiments: Raster, sequence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "651fca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset import QuickDrawDataset\n",
    "from utils import AbsolutePenPositionTokenizer\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0c0017c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d0dd2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading QuickDraw files: 100%|██████████| 1/1 [00:00<00:00, 1136.36it/s]\n",
      "Loading QuickDraw files: 1it [00:03,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenized data from sketch-im_tokenized_dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "import pyvips\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def svg_rasterize(svg_string: str, size=(64, 64)) -> Image.Image:\n",
    "    svg_bytes = svg_string.encode(\"utf-8\")\n",
    "    image = pyvips.Image.svgload_buffer(svg_bytes)\n",
    "    image = image.flatten(background=0xFFFFFF)\n",
    "    png_bytes = image.write_to_buffer(\".png\")\n",
    "    img = Image.open(BytesIO(png_bytes)).convert(\"L\")\n",
    "    img = img.resize(size)\n",
    "    tensor = torch.tensor(list(img.getdata()), dtype=torch.float32)\n",
    "    tensor = tensor.view(size[1], size[0]) / 255.0\n",
    "    return tensor\n",
    "\n",
    "\n",
    "labels = [\"cat\"]\n",
    "\n",
    "training_data = QuickDrawDataset(labels=labels, download=True)\n",
    "\n",
    "tokenizer = AbsolutePenPositionTokenizer(bins=64)\n",
    "\n",
    "\n",
    "class SketchReconDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        svg_list,\n",
    "        tokenizer,\n",
    "        max_len=200,\n",
    "        cache_file=\"sketch-im_tokenized_dataset.pkl\",\n",
    "    ):\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.pad_id = tokenizer.vocab[\"PAD\"]\n",
    "\n",
    "        # Try to load from cache\n",
    "        try:\n",
    "            with open(cache_file, \"rb\") as f:\n",
    "                self.data = pickle.load(f)\n",
    "            print(f\"Loaded tokenized data from {cache_file}\")\n",
    "        except FileNotFoundError:\n",
    "            for svg in tqdm(svg_list, desc=\"Tokenizing SVGs\"):\n",
    "                tokens = tokenizer.encode(svg)[:max_len]\n",
    "                tokens = tokens + [self.pad_id] * (max_len - len(tokens))\n",
    "                image = svg_rasterize(svg)\n",
    "                self.data.append((tokens, image))\n",
    "\n",
    "            with open(cache_file, \"wb\") as f:\n",
    "                pickle.dump(self.data, f)\n",
    "            print(f\"Saved tokenized data to {cache_file}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq, image = self.data[idx]\n",
    "        input_ids = torch.tensor(seq[:-1])\n",
    "        target_ids = torch.tensor(seq[1:])\n",
    "        return image, input_ids, target_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "dataset = SketchReconDataset(training_data, tokenizer, max_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36e3298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, input_ids, target_ids = dataset[0]\n",
    "\n",
    "image = (image * 255).byte()\n",
    "\n",
    "# show image\n",
    "image = image.numpy()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "398f5725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting piq\n",
      "  Downloading piq-0.8.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: torchvision>=0.10.0 in c:\\users\\matth\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from piq) (0.23.0+cu128)\n",
      "Requirement already satisfied: numpy in c:\\users\\matth\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision>=0.10.0->piq) (1.26.4)\n",
      "Requirement already satisfied: torch==2.8.0+cu128 in c:\\users\\matth\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision>=0.10.0->piq) (2.8.0+cu128)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\matth\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision>=0.10.0->piq) (11.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\matth\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.8.0+cu128->torchvision>=0.10.0->piq) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\matth\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.8.0+cu128->torchvision>=0.10.0->piq) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\matth\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.8.0+cu128->torchvision>=0.10.0->piq) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\matth\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.8.0+cu128->torchvision>=0.10.0->piq) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\matth\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.8.0+cu128->torchvision>=0.10.0->piq) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\matth\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.8.0+cu128->torchvision>=0.10.0->piq) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\matth\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.8.0+cu128->torchvision>=0.10.0->piq) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\matth\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch==2.8.0+cu128->torchvision>=0.10.0->piq) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\matth\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch==2.8.0+cu128->torchvision>=0.10.0->piq) (2.1.5)\n",
      "Downloading piq-0.8.0-py3-none-any.whl (106 kB)\n",
      "Installing collected packages: piq\n",
      "Successfully installed piq-0.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install piq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63ef7bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: default feature extractor (InceptionNet V2) is used.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\matth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\matth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\models\\inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\" to C:\\Users\\matth/.cache\\torch\\hub\\checkpoints\\pt_inception-2015-12-05-6726825d.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91.2M/91.2M [00:03<00:00, 24.2MB/s]\n",
      "c:\\Users\\matth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\piq\\base.py:44: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\python_variable_indexing.cpp:312.)\n",
      "  images = batch['images']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m first_dl, second_dl = DataLoader(sketch_images, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m), DataLoader(sketch_images)\n\u001b[32m     18\u001b[39m fid_metric = FID()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m first_feats = \u001b[43mfid_metric\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_feats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m second_feats = fid_metric.compute_feats(second_dl)\n\u001b[32m     21\u001b[39m fid: torch.Tensor = fid_metric(first_feats, second_feats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\piq\\base.py:44\u001b[39m, in \u001b[36mBaseFeatureMetric.compute_feats\u001b[39m\u001b[34m(self, loader, feature_extractor, device)\u001b[39m\n\u001b[32m     42\u001b[39m total_feats = []\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     images = \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimages\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     45\u001b[39m     N = images.shape[\u001b[32m0\u001b[39m]\n\u001b[32m     46\u001b[39m     images = images.float().to(device)\n",
      "\u001b[31mIndexError\u001b[39m: too many indices for tensor of dimension 3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from piq import FID\n",
    "\n",
    "class SketchImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx][0]\n",
    "    \n",
    "sketch_images = SketchImageDataset(dataset)\n",
    "\n",
    "first_dl, second_dl = DataLoader(sketch_images, shuffle=True), DataLoader(sketch_images)\n",
    "fid_metric = FID()\n",
    "first_feats = fid_metric.compute_feats(first_dl)\n",
    "second_feats = fid_metric.compute_feats(second_dl)\n",
    "fid: torch.Tensor = fid_metric(first_feats, second_feats)\n",
    "\n",
    "print(f\"FID: {fid:0.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40822314",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs: torch.Tensor = piq.GS(sample_size=64, num_iters=100, i_max=100, num_workers=4)(x_features, y_features)\n",
    "print(f\"GS: {gs:0.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
