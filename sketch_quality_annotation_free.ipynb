{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef72235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.n_layers = 2\n",
    "        self.num_worker = 8\n",
    "        self.d_feed = 1024\n",
    "        self.dropout = 0.2\n",
    "        self.l_a = 10\n",
    "        self.l_m = 0.3\n",
    "        self.u_a = 100\n",
    "        self.u_m = 0.6\n",
    "        self.scale = 64\n",
    "        self.tau = 1e-1\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=5, hidden_size=config.d_feed,\n",
    "                            num_layers=config.n_layers, bidirectional=True,\n",
    "                            batch_first=True, dropout=config.dropout)\n",
    "\n",
    "        self.classifer = nn.Linear(config.d_feed, config.num_classes)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.norm = nn.BatchNorm1d(config.d_feed)\n",
    "        self.fc = nn.Linear(config.d_feed * 2, config.d_feed)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x, lens):\n",
    "        _, idx_sort = torch.sort(lens, dim=0, descending=True)\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "\n",
    "        input_x = x.index_select(0, Variable(idx_sort))\n",
    "        length_list = list(lens[idx_sort])\n",
    "        pack = nn.utils.rnn.pack_padded_sequence(input_x, length_list, batch_first=True)\n",
    "        out, state = self.lstm(pack)\n",
    "        un_padded = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        un_padded = un_padded[0].index_select(0, Variable(idx_unsort))\n",
    "        out = self.dropout(un_padded)\n",
    "\n",
    "        lens_idx = lens - 1\n",
    "        lens_list = lens_idx.cpu().int().tolist()\n",
    "        feats = out[torch.arange(x.size(0)), lens_list, :]\n",
    "\n",
    "        return self.activation(self.fc(self.dropout(feats)))\n",
    "\n",
    "class GACL(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, config):\n",
    "        super(GACL, self).__init__()\n",
    "        self.u_a = config.u_a\n",
    "        self.u_m = config.u_m\n",
    "        self.l_a = config.l_a\n",
    "        self.l_m = config.l_m\n",
    "        k = (self.u_m - self.l_m) / (self.u_a - self.l_a)\n",
    "        self.min_lamada = config.scale * k * self.u_a ** 2 * self.l_a ** 2 / (self.u_a ** 2 - self.l_a ** 2)\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(output_dim, input_dim))\n",
    "        nn.init.kaiming_uniform_(self.weight)\n",
    "        self.scale = config.scale\n",
    "        self.tau = config.tau\n",
    "\n",
    "    def calc_loss_G(self, x_norm):\n",
    "        g = 1/(self.u_a**2) * x_norm + 1/(x_norm)\n",
    "        return torch.mean(g)\n",
    "\n",
    "    def get_margin(self, x):\n",
    "        margin = (self.u_m - self.l_m) / \\\n",
    "                 (self.u_a - self.l_a) * (x - self.l_a) + self.l_m\n",
    "        return margin\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        batch_size = input.size(0)\n",
    "        x_norm = torch.norm(input, 2, 1)#.clamp(self.l_a, self.u_a)\n",
    "        loss_g = self.calc_loss_G(x_norm) # calculate g(a)\n",
    "        cos_theta = F.linear(F.normalize(input), F.normalize(self.weight)).clamp(-1, 1)\n",
    "        target_cos = cos_theta[range(batch_size), target]\n",
    "\n",
    "        ada_margin = self.get_margin(x_norm)  # calculate m(a)\n",
    "        adjusted_margin = ada_margin\n",
    "        adjusted_margin = adjusted_margin.clamp(self.l_m, self.u_m)\n",
    "        GACL_cos = self.m4(target_cos, adjusted_margin)\n",
    "\n",
    "        if self.training:\n",
    "            preds_ = cos_theta\n",
    "            preds_[range(batch_size), target] = torch.squeeze(GACL_cos)  # replace the y_i from (cos theta) to (cos theta + m)\n",
    "\n",
    "            return self.scale * preds_, loss_g, x_norm\n",
    "        else:\n",
    "            preds_ = cos_theta/self.scale\n",
    "            return preds_, loss_g, x_norm, preds_[range(batch_size), target]\n",
    "\n",
    "    def m4(self, cos_theta, margin):\n",
    "        return cos_theta - margin"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
