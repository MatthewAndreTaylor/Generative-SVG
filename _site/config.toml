[[params.authors]]
name = "Matthew Taylor"
url  = "https://matthewandretaylor.github.io"

[[params.authors]]
name = "Sebastian Tasson"
url  = "https://github.com/tassonse"


[[params.models]]
tag = "small"
checkpoint = "model_checkpoint_small.pt"
labels = ["bird", "crab", "guitar"]

[[params.models]]
tag = "medium"
checkpoint = "model_checkpoint_medium.pt"
labels = ["cake", "butterfly", "flower", "mug", "sea turtle"]

[params]
title = 'Conditional Sketch Generation & Completion'
pdf_url  = "static/paper.pdf"
code_url = "https://github.com/MatthewAndreTaylor/Generative-SVG"

abstract = """
Existing sketch generation models rely on hand-crafted stroke representations that are not very generalizable across datasets or drawing styles, and train multiple models to generate sketches over different object classes.
These issues make it challenging to embed deeper semantic knowledge of sketches or scale to a variety of classes.
Furthermore, the temporal and structural information present in human-drawn sketches is frequently lost by conventional raster-based methods.
We present a new system for conditional sketch generation and completion that uses a novel quantization and tokenization technique before encoding sketches into discrete token sequences.
To improve data quality, we simplify input paths and to keep the quality of outputs we fit the strokes to bezier curves.
We train a lightweight transformer model with an added class embedding layer to learn from multiple sketch object classes at once. This allows a single model to generalize across classes and reuse learned shape priors.
"""

representation = """
We represent a sketch as a sequence of strokes ...
"""

model = """
Our model is a transformer-based architecture ...
"""

training = """
We train our model on the Google Quick, Draw! dataset ...
"""