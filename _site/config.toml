[[params.authors]]
name = "Matthew Taylor"
url  = "https://matthewandretaylor.github.io"

[[params.authors]]
name = "Sebastian Tasson"
url  = "https://github.com/tassonse"


[[params.models]]
tag = "small"
checkpoint = "model_checkpoint_small.pt"
labels = ["bird", "crab", "guitar"]

[[params.models]]
tag = "medium"
checkpoint = "model_checkpoint_medium.pt"
labels = ["cake", "butterfly", "flower", "mug", "sea turtle"]

[params]
title = 'Conditional Sketch Generation & Completion'
pdf_url  = "static/paper.pdf"
code_url = "https://github.com/MatthewAndreTaylor/Generative-SVG"

abstract = """
Existing sketch generation models rely on hand-crafted stroke representations that are not very generalizable across datasets or drawing styles, and train multiple models to generate sketches over different object classes.
These issues make it challenging to embed deeper semantic knowledge of sketches or scale to a variety of classes.
Furthermore, the temporal and structural information present in human-drawn sketches is frequently lost by conventional raster-based methods.
We present a new system for conditional sketch generation and completion that uses a novel quantization and tokenization technique before encoding sketches into discrete token sequences.
To improve data quality, we simplify input paths and to keep the quality of outputs we fit the strokes to bezier curves.
We train a lightweight transformer model with an added class embedding layer to learn from multiple sketch object classes at once. This allows a single model to generalize across classes and reuse learned shape priors.
"""

representation = """
We represent a sketch as a sequence of strokes. Each stroke is quantized and encoded as a discrete token using a fix-sized codebook.
This representation captures both the temporal and structural information present in human-drawn sketches, allowing our model to generate realistic and coherent sketches.
"""

model = """
We designed our model ourselves it is based on a transformer architecture, but does not follow any existing implementation.
In addition, the class embedding allows the model to condition its generation on specific object classes, enabling it to generate sketches of different types using a single model.
We use fewer layers and attention heads compared to standard transformer architectures to keep the model lightweight.
"""

training = """
As training input we use the tokenized sketch representation along with a class embedding.
We train our model using Cross-entropy loss, and the AdamW optimizer.
This way we can effectively learn the distribution of sketch tokens and generate sketch sequences based on the learned patterns.
"""