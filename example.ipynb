{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971d78e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2c6576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import QuickDrawDataset\n",
    "from sketch_tokenizers import DeltaPenPositionTokenizer\n",
    "from models import SketchTransformerConditional\n",
    "from runner import SketchTrainer, sample, device\n",
    "import torch\n",
    "\n",
    "if device == \"cuda\":\n",
    "    # Empty the cache to free up GPU memory\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d791bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"bird\", \"crab\", \"guitar\"]\n",
    "dataset = QuickDrawDataset(label_names=label_names)\n",
    "tokenizer = DeltaPenPositionTokenizer(bins=32)\n",
    "\n",
    "model = SketchTransformerConditional(\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_layers=8,\n",
    "    max_len=200,\n",
    "    num_classes=len(label_names),\n",
    ")\n",
    "\n",
    "training_config = {\n",
    "    \"batch_size\": 128,\n",
    "    \"num_epochs\": 15,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"log_dir\": \"logs/sketch_transformer_experiment_2\",\n",
    "    \"splits\": [0.85, 0.1, 0.05],\n",
    "}\n",
    "\n",
    "trainer = SketchTrainer(model, dataset, tokenizer, training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b32610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_mixed(training_config[\"num_epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "from prepare_data import stroke_to_bezier_single, clean_svg\n",
    "\n",
    "generations_inline = \"\"\n",
    "generations = []\n",
    "\n",
    "for j, label_name in enumerate(label_names):\n",
    "    for i in range(5):\n",
    "        generated = sample(\n",
    "            model=trainer.model,\n",
    "            start_tokens=[trainer.tokenizer.vocab[\"START\"]],\n",
    "            temperature=0.8,\n",
    "            top_k=20,\n",
    "            top_p=0.7,\n",
    "            greedy=False,\n",
    "            eos_id=trainer.tokenizer.vocab[\"END\"],\n",
    "            class_label=j,\n",
    "        )\n",
    "        decoded_sketch = tokenizer.decode(generated, stroke_width=0.3)\n",
    "        decoded_sketch = stroke_to_bezier_single(decoded_sketch)\n",
    "        decoded_sketch = clean_svg(decoded_sketch)\n",
    "\n",
    "        generations_inline += f'<div style=\"display:inline-block; width: 150px; background-color: white; margin-right:10px;\"><b>Generated {label_name}</b><br>{decoded_sketch}</div>'\n",
    "        generations.append((generated, decoded_sketch))\n",
    "\n",
    "display(HTML(generations_inline))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
