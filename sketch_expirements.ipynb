{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651fca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset import QuickDrawDataset\n",
    "from utils import svg_strokes_to_tensor_quantized, tensor_to_svg_strokes\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0c0017c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d0dd2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"cat\", \"dog\", \"fish\", \"airplane\"]\n",
    "\n",
    "\n",
    "def to_tensor_int8(svg_content):\n",
    "    return svg_strokes_to_tensor_quantized(svg_content).to(dtype=torch.int8)\n",
    "\n",
    "\n",
    "training_data = QuickDrawDataset(\n",
    "    labels=labels,\n",
    "    base_transform=svg_strokes_to_tensor_quantized,\n",
    "    cache_file=\"data/quickdraw_all_quantized_int8.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46329416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "508559\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))\n",
    "loader = DataLoader(training_data, batch_size=32, shuffle=False, pin_memory=True)\n",
    "loader = cycle(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b12413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_strokes(batch, scale=128.0):\n",
    "    batch = batch.clone().float()\n",
    "    # Normalize delta x, delta y\n",
    "    batch[..., 0:2] = batch[..., 0:2] / scale\n",
    "    # Leave pen_state as is (0, 1, -1)\n",
    "    return batch\n",
    "\n",
    "\n",
    "def denormalize_strokes(batch, scale=128.0):\n",
    "    batch = batch.clone()\n",
    "    batch[..., 0:2] = batch[..., 0:2] * scale\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "053aefe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5000,  0.2812,  0.0000],\n",
      "        [-0.0625, -0.1250,  1.0000],\n",
      "        [-0.0547, -0.0547,  1.0000],\n",
      "        [ 0.0391,  0.2031,  1.0000],\n",
      "        [-0.1250,  0.0156,  1.0000]], device='cuda:0')\n",
      "torch.Size([32, 200, 3])\n"
     ]
    }
   ],
   "source": [
    "# Test normalization and denormalization\n",
    "batch = next(loader)\n",
    "batch = batch.to(device)\n",
    "batch = normalize_strokes(batch)\n",
    "print(batch[0][0:5])\n",
    "\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3c80d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_padding_mask(x):\n",
    "    \"\"\"\n",
    "    Create a padding mask for continuous stroke data.\n",
    "    Goal is to ignore padding in the attention mechanism, only attend to real data (strokes).\n",
    "    Padding assumed to be all zeros [0,0,0].\n",
    "    Any row with all zeros is considered padding.\n",
    "    x: (batch, seq_len, input_dim)\n",
    "    returns: (batch, seq_len) boolean mask\n",
    "    \"\"\"\n",
    "    return x.abs().sum(dim=-1) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8f1adb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 1000):\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute positional encodings once in log space.\n",
    "        position = torch.arange(max_len).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )  # (d_model/2,)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # odd indices\n",
    "\n",
    "        self.register_buffer(\"pe\", pe, persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:seq_len, :].unsqueeze(0)  # broadcast across batch\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model, num_heads=num_heads, batch_first=True\n",
    "        )\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dff), nn.ReLU(), nn.Linear(dff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention\n",
    "        attn_output, _ = self.self_attn(\n",
    "            x, x, x, key_padding_mask=mask, need_weights=False\n",
    "        )\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "\n",
    "        # Feed-forward\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout2(ffn_output))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        dff,\n",
    "        input_dim,\n",
    "        dropout,\n",
    "        seq_len,\n",
    "        use_continuous_input=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        if use_continuous_input:\n",
    "            self.embedding = nn.Linear(input_dim, d_model)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(input_dim, d_model)\n",
    "\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len=seq_len)\n",
    "\n",
    "        self.enc_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, dff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = make_padding_mask(x)  # (batch, seq_len)\n",
    "        x = self.embedding(x)  # (batch, seq_len, d_model)\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "\n",
    "        x = self.pos_encoding(x)  # add positional encoding\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x, mask=mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edfd3f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 200, 128])\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(\n",
    "    num_layers=4,  # Number of encoder layers\n",
    "    d_model=128,  # Dimensionality of the embeddings and hidden states (the size of the vector representing each token).\n",
    "    num_heads=8,  # Number of attention heads in the multi-head self-attention mechanism. Each head learns to focus on different types of relationships between tokens.\n",
    "    dff=256,  # Dimensionality of the feed-forward network inside each encoder layer. Typically larger than d_model (e.g., d_model=512, dff=2048) for expressiveness.\n",
    "    input_dim=3,  # input_dim = 3 (x, y, pen)\n",
    "    dropout=0.1,\n",
    "    seq_len=200,  # Maximum sequence length\n",
    "    use_continuous_input=True,\n",
    ").to(device)\n",
    "\n",
    "out = encoder(batch)  # batch: (32, 200, 3)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5644f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dff, d_model),\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        enc_output,\n",
    "        tgt_mask=None,\n",
    "        memory_mask=None,\n",
    "        tgt_key_padding_mask=None,\n",
    "        memory_key_padding_mask=None,\n",
    "    ):\n",
    "        # 1. masked self-attention\n",
    "        _x, _ = self.self_attn(\n",
    "            x,\n",
    "            x,\n",
    "            x,\n",
    "            attn_mask=tgt_mask,\n",
    "            key_padding_mask=tgt_key_padding_mask,\n",
    "            need_weights=False,\n",
    "        )\n",
    "        x = self.norm1(x + self.dropout1(_x))\n",
    "\n",
    "        # 2. cross-attention\n",
    "        _x, _ = self.cross_attn(\n",
    "            x,\n",
    "            enc_output,\n",
    "            enc_output,\n",
    "            attn_mask=memory_mask,\n",
    "            key_padding_mask=memory_key_padding_mask,\n",
    "            need_weights=False,\n",
    "        )\n",
    "        x = self.norm2(x + self.dropout2(_x))\n",
    "\n",
    "        # 3. feed-forward\n",
    "        _x = self.ffn(x)\n",
    "        x = self.norm3(x + self.dropout3(_x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        dff,\n",
    "        target_dim,\n",
    "        maximum_position_encoding=1000,\n",
    "        dropout=0.1,\n",
    "        use_continuous_input=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        if use_continuous_input:\n",
    "            self.embedding = nn.Linear(target_dim, d_model)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(target_dim, d_model)\n",
    "\n",
    "        self.pos_encoding = PositionalEncoding(d_model, maximum_position_encoding)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, num_heads, dff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        enc_output,\n",
    "        tgt_mask=None,\n",
    "        memory_mask=None,\n",
    "        tgt_key_padding_mask=None,\n",
    "        memory_key_padding_mask=None,\n",
    "    ):\n",
    "        x = self.embedding(x)\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x,\n",
    "                enc_output,\n",
    "                tgt_mask=tgt_mask,\n",
    "                memory_mask=memory_mask,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                memory_key_padding_mask=memory_key_padding_mask,\n",
    "            )\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        dff,\n",
    "        src_dim,\n",
    "        tgt_dim,\n",
    "        dropout=0.1,\n",
    "        use_continuous_input=True,\n",
    "        seq_len=200,\n",
    "        output_dim=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            num_layers=num_encoder_layers,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dff=dff,\n",
    "            input_dim=src_dim,\n",
    "            dropout=dropout,\n",
    "            use_continuous_input=use_continuous_input,\n",
    "            seq_len=seq_len,\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            num_layers=num_decoder_layers,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dff=dff,\n",
    "            target_dim=tgt_dim,  # used for embedding/input dim\n",
    "            dropout=dropout,\n",
    "            use_continuous_input=use_continuous_input,\n",
    "        )\n",
    "\n",
    "        # Projection to output dimension\n",
    "        self.output_layer = nn.Linear(d_model, output_dim or tgt_dim)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        enc_output = self.encoder(src)  # (batch, src_seq_len, d_model)\n",
    "        dec_output = self.decoder(tgt, enc_output)  # (batch, tgt_seq_len, d_model)\n",
    "        return self.output_layer(dec_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bd8ff99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 200, 3])\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(\n",
    "    num_encoder_layers=4,\n",
    "    num_decoder_layers=4,\n",
    "    d_model=128,\n",
    "    num_heads=8,\n",
    "    dff=256,\n",
    "    src_dim=3,  # input_dim = 3 (x, y, pen)\n",
    "    tgt_dim=3,  # target_dim = 3 (x, y, pen\n",
    "    dropout=0.1,\n",
    "    seq_len=200,\n",
    "    use_continuous_input=True,\n",
    ").to(device)\n",
    "\n",
    "batch = next(loader)  # e.g. (32, 200, 3)\n",
    "batch = normalize_strokes(batch)  # preprocessing\n",
    "batch = batch.to(device)\n",
    "\n",
    "out = model(batch, batch)  # teacher-forcing: predict batch from itself\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01ef924f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/15000 [00:00<25:24,  9.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss=0.822011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1005/15000 [00:33<07:46, 30.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000: loss=0.000984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2006/15000 [01:06<07:17, 29.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000: loss=0.000450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3004/15000 [01:40<06:39, 30.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3000: loss=0.000303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4006/15000 [02:13<06:07, 29.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4000: loss=0.000205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5005/15000 [02:47<05:34, 29.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5000: loss=0.000182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6006/15000 [03:20<05:01, 29.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6000: loss=0.000123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7006/15000 [03:54<04:27, 29.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7000: loss=0.000118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 7441/15000 [04:08<04:12, 29.90it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m data = normalize_strokes(data).to(device)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Forward: predict next strokes given input\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m recon = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Reconstruction loss\u001b[39;00m\n\u001b[32m     16\u001b[39m loss = F.mse_loss(recon, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, src, tgt)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, tgt):\n\u001b[32m     39\u001b[39m     enc_output = \u001b[38;5;28mself\u001b[39m.encoder(src)              \u001b[38;5;66;03m# (batch, src_seq_len, d_model)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     dec_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch, tgt_seq_len, d_model)\u001b[39;00m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output_layer(dec_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 76\u001b[39m, in \u001b[36mDecoder.forward\u001b[39m\u001b[34m(self, x, enc_output, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[39m\n\u001b[32m     73\u001b[39m x = \u001b[38;5;28mself\u001b[39m.dropout(x)\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mDecoderLayer.forward\u001b[39m\u001b[34m(self, x, enc_output, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, enc_output, tgt_mask=\u001b[38;5;28;01mNone\u001b[39;00m, memory_mask=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     22\u001b[39m             tgt_key_padding_mask=\u001b[38;5;28;01mNone\u001b[39;00m, memory_key_padding_mask=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# 1. masked self-attention\u001b[39;00m\n\u001b[32m     24\u001b[39m     _x, _ = \u001b[38;5;28mself\u001b[39m.self_attn(\n\u001b[32m     25\u001b[39m         x, x, x,\n\u001b[32m     26\u001b[39m         attn_mask=tgt_mask,\n\u001b[32m     27\u001b[39m         key_padding_mask=tgt_key_padding_mask,\n\u001b[32m     28\u001b[39m         need_weights=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     29\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_x\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# 2. cross-attention\u001b[39;00m\n\u001b[32m     33\u001b[39m     _x, _ = \u001b[38;5;28mself\u001b[39m.cross_attn(\n\u001b[32m     34\u001b[39m         x, enc_output, enc_output,\n\u001b[32m     35\u001b[39m         attn_mask=memory_mask,\n\u001b[32m     36\u001b[39m         key_padding_mask=memory_key_padding_mask,\n\u001b[32m     37\u001b[39m         need_weights=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     38\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1769\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1766\u001b[39m             tracing_state.pop_scope()\n\u001b[32m   1767\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m-> \u001b[39m\u001b[32m1769\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1770\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1771\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "num_training_updates = 15000\n",
    "train_losses = []\n",
    "\n",
    "model.train()\n",
    "for i in tqdm(range(num_training_updates)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    data = next(loader)  # (batch, seq_len, 3)\n",
    "    data = normalize_strokes(data).to(device)\n",
    "\n",
    "    # Forward: predict next strokes given input\n",
    "    recon = model(data, data)\n",
    "\n",
    "    # Reconstruction loss\n",
    "    loss = F.mse_loss(recon, data)\n",
    "\n",
    "    # Backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Step {i}: loss={loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac7a26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Smooth training losses\n",
    "train_losses_smooth = savgol_filter(train_losses, 201, 7)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(train_losses_smooth)\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Smoothed Reconstruction Loss (MSE)\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddc3fac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display:inline-block; width: 150px; background-color: white; margin-right:10px;\"><svg viewBox=\"0 0 128 128\"><g stroke-width=\"0.8\">\n",
       "<path d=\"M 64.0 36.0 L 56.0 20.0 L 49.0 13.0 L 54.0 39.0 L 38.0 41.0 L 32.0 44.0 L 27.0 50.0 L 24.0 60.0 L 24.0 66.0 L 25.0 75.0 L 29.0 82.0 L 43.0 91.0 L 66.0 94.0 L 76.0 92.0 L 84.0 89.0 L 101.0 75.0 L 106.0 65.0 L 108.0 57.0 L 107.0 50.0 L 103.0 44.0 L 92.0 38.0 L 87.0 0.0 L 80.0 15.0 L 78.0 32.0 L 65.0 35.0\" stroke=\"black\" fill=\"none\"/>\n",
       "<path d=\"M 38.0 67.0 L 14.0 64.0 L 3.0 64.0\" stroke=\"black\" fill=\"none\"/>\n",
       "<path d=\"M 38.0 79.0 L 11.0 81.0 L 0.0 87.0\" stroke=\"black\" fill=\"none\"/>\n",
       "<path d=\"M 43.0 87.0 L 26.0 95.0 L 18.0 101.0\" stroke=\"black\" fill=\"none\"/>\n",
       "<path d=\"M 86.0 66.0 L 109.0 65.0 L 122.0 67.0 L 125.0 69.0\" stroke=\"black\" fill=\"none\"/>\n",
       "<path d=\"M 87.0 73.0 L 127.0 83.0\" stroke=\"black\" fill=\"none\"/>\n",
       "<path d=\"M 85.0 81.0 L 103.0 98.0 L 107.0 104.0\" stroke=\"black\" fill=\"none\"/>\n",
       "<path d=\"M 64.0 64.0 L 55.0 66.0 L 54.0 68.0 L 55.0 71.0 L 64.0 73.0 L 69.0 71.0 L 69.0 67.0 L 59.0 64.0\" stroke=\"black\" fill=\"none\"/>\n",
       "<path d=\"M 53.0 48.0 L 53.0 56.0\" stroke=\"black\" fill=\"none\"/>\n",
       "</g></svg></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated shape: torch.Size([1, 200, 3])\n",
      "tensor([[ 6.4000e+01,  3.6000e+01,  0.0000e+00],\n",
      "        [-8.0000e+00, -1.6000e+01,  1.0000e+00],\n",
      "        [-7.0000e+00, -7.0000e+00,  1.0000e+00],\n",
      "        [ 5.0000e+00,  2.6000e+01,  1.0000e+00],\n",
      "        [-1.6000e+01,  2.0000e+00,  1.0000e+00],\n",
      "        [-1.5140e+01, -3.5558e+00,  9.7433e-01],\n",
      "        [-1.3726e+01, -7.5254e+00,  9.4237e-01],\n",
      "        [-1.1286e+01, -1.0032e+01,  8.9654e-01],\n",
      "        [-8.8512e+00, -1.1762e+01,  8.3797e-01],\n",
      "        [-6.7355e+00, -1.2990e+01,  7.6832e-01],\n",
      "        [-4.7879e+00, -1.4557e+01,  6.8783e-01],\n",
      "        [-2.8483e+00, -1.6502e+01,  5.9102e-01],\n",
      "        [-5.4212e-01, -1.8529e+01,  4.4752e-01],\n",
      "        [ 2.5557e+00, -2.0237e+01,  2.2039e-01],\n",
      "        [ 5.7199e+00, -1.8424e+01,  1.5151e-02],\n",
      "        [ 6.0012e+00, -1.3851e+01, -1.7995e-03],\n",
      "        [ 5.6980e+00, -8.0621e+00, -1.8364e-03],\n",
      "        [ 4.3594e+00, -2.5518e+00, -1.7585e-03],\n",
      "        [ 2.0384e+00,  5.5226e-01, -2.1656e-03],\n",
      "        [ 5.3649e-01,  1.2860e+00, -2.2895e-03],\n",
      "        [ 1.1651e-01,  1.4563e+00, -2.2297e-03],\n",
      "        [-5.4764e-02,  1.2873e+00, -2.2171e-03],\n",
      "        [-2.8773e-01,  8.5595e-01, -2.0680e-03],\n",
      "        [-4.1044e-01,  7.8173e-01, -1.8409e-03],\n",
      "        [-5.1755e-01,  7.4439e-01, -1.5542e-03],\n",
      "        [-5.7285e-01,  7.1741e-01, -1.2781e-03],\n",
      "        [-5.6421e-01,  7.0998e-01, -1.0529e-03],\n",
      "        [-5.2514e-01,  7.0411e-01, -1.0237e-03],\n",
      "        [-5.0442e-01,  7.0141e-01, -1.0929e-03],\n",
      "        [-5.2674e-01,  6.9729e-01, -1.1924e-03],\n",
      "        [-5.7083e-01,  6.8119e-01, -1.2812e-03],\n",
      "        [-5.9448e-01,  6.5852e-01, -1.3126e-03],\n",
      "        [-5.6884e-01,  6.3838e-01, -1.2520e-03],\n",
      "        [-4.9864e-01,  6.3768e-01, -1.0759e-03],\n",
      "        [-4.5445e-01,  6.6218e-01, -8.6179e-04],\n",
      "        [-4.2908e-01,  6.9899e-01, -7.5951e-04],\n",
      "        [-4.3042e-01,  7.2362e-01, -7.6091e-04],\n",
      "        [-4.6489e-01,  7.1409e-01, -8.8271e-04],\n",
      "        [-5.1648e-01,  6.7271e-01, -1.1110e-03],\n",
      "        [-5.5930e-01,  6.2312e-01, -1.2340e-03],\n",
      "        [-5.7995e-01,  5.9246e-01, -1.1827e-03],\n",
      "        [-5.9545e-01,  5.8499e-01, -9.6097e-04],\n",
      "        [-6.1992e-01,  5.9370e-01, -6.5508e-04],\n",
      "        [-6.4599e-01,  6.0064e-01, -3.6776e-04],\n",
      "        [-6.5983e-01,  5.9119e-01, -2.2387e-04],\n",
      "        [-6.5052e-01,  5.6242e-01, -2.3621e-04],\n",
      "        [-6.1365e-01,  5.1922e-01, -3.7894e-04],\n",
      "        [-5.8033e-01,  4.7527e-01, -5.4425e-04],\n",
      "        [-5.9012e-01,  4.3552e-01, -7.1415e-04],\n",
      "        [-6.0647e-01,  3.9962e-01, -7.3784e-04],\n",
      "        [-6.0810e-01,  3.6638e-01, -6.0362e-04],\n",
      "        [-5.9093e-01,  3.3968e-01, -3.5322e-04],\n",
      "        [-5.8411e-01,  3.2514e-01, -1.0294e-04],\n",
      "        [-5.6163e-01,  3.2020e-01,  5.0456e-05],\n",
      "        [-5.2505e-01,  3.1864e-01,  6.3718e-05],\n",
      "        [-4.9301e-01,  3.0374e-01, -4.4911e-05],\n",
      "        [-4.7165e-01,  2.7061e-01, -2.2513e-04],\n",
      "        [-4.7172e-01,  2.3203e-01, -3.8505e-04],\n",
      "        [-4.9048e-01,  2.1274e-01, -4.2713e-04],\n",
      "        [-5.1113e-01,  2.3091e-01, -2.9471e-04],\n",
      "        [-5.3521e-01,  2.8374e-01, -8.1061e-05],\n",
      "        [-5.7364e-01,  3.4156e-01, -2.6613e-05],\n",
      "        [-6.1578e-01,  3.7533e-01, -1.8531e-04],\n",
      "        [-6.4419e-01,  3.6905e-01, -4.3362e-04],\n",
      "        [-6.4295e-01,  3.3540e-01, -5.6437e-04],\n",
      "        [-6.4110e-01,  2.9978e-01, -6.1002e-04],\n",
      "        [-6.6566e-01,  2.8200e-01, -4.9850e-04],\n",
      "        [-7.0926e-01,  2.8963e-01, -3.1161e-04],\n",
      "        [-7.5026e-01,  3.0634e-01, -1.3286e-04],\n",
      "        [-7.6915e-01,  3.1543e-01,  8.6436e-06],\n",
      "        [-7.4869e-01,  3.0841e-01,  1.0750e-04],\n",
      "        [-6.9074e-01,  2.8542e-01,  2.3100e-04],\n",
      "        [-6.3265e-01,  2.5226e-01,  2.5991e-04],\n",
      "        [-6.0408e-01,  2.2276e-01,  2.5910e-04],\n",
      "        [-6.1086e-01,  2.0122e-01,  3.1516e-04],\n",
      "        [-6.3493e-01,  1.9715e-01,  3.7959e-04],\n",
      "        [-6.4551e-01,  2.1117e-01,  4.3660e-04],\n",
      "        [-6.3184e-01,  2.3377e-01,  4.8980e-04],\n",
      "        [-6.0523e-01,  2.5167e-01,  5.3972e-04],\n",
      "        [-5.8927e-01,  2.6752e-01,  5.2768e-04],\n",
      "        [-6.0182e-01,  2.7082e-01,  4.3020e-04],\n",
      "        [-6.2881e-01,  2.5727e-01,  3.1313e-04],\n",
      "        [-6.5315e-01,  2.3563e-01,  2.8601e-04],\n",
      "        [-6.6425e-01,  2.1422e-01,  3.7095e-04],\n",
      "        [-6.5386e-01,  1.9923e-01,  4.8196e-04],\n",
      "        [-6.3999e-01,  1.9245e-01,  5.8887e-04],\n",
      "        [-6.3751e-01,  1.8829e-01,  5.9447e-04],\n",
      "        [-6.5017e-01,  1.7689e-01,  4.5005e-04],\n",
      "        [-6.6412e-01,  1.5297e-01,  2.3767e-04],\n",
      "        [-6.6749e-01,  1.2529e-01,  1.5867e-04],\n",
      "        [-6.4118e-01,  1.1660e-01,  2.6497e-04],\n",
      "        [-5.9674e-01,  1.2950e-01,  5.4026e-04],\n",
      "        [-5.5148e-01,  1.5298e-01,  8.5223e-04],\n",
      "        [-5.2062e-01,  1.7431e-01,  1.1131e-03],\n",
      "        [-5.1488e-01,  1.8703e-01,  1.2403e-03],\n",
      "        [-5.2608e-01,  1.8055e-01,  1.1438e-03],\n",
      "        [-5.2689e-01,  1.6871e-01,  9.5809e-04],\n",
      "        [-5.1910e-01,  1.5557e-01,  7.6878e-04],\n",
      "        [-5.2234e-01,  1.4044e-01,  5.3912e-04],\n",
      "        [-5.4601e-01,  1.2012e-01,  3.1898e-04],\n",
      "        [-5.6974e-01,  9.6563e-02,  3.0774e-04],\n",
      "        [-5.8893e-01,  7.7747e-02,  4.7863e-04],\n",
      "        [-6.0107e-01,  7.1206e-02,  6.8808e-04],\n",
      "        [-6.0538e-01,  8.2133e-02,  8.1727e-04],\n",
      "        [-6.0395e-01,  1.0175e-01,  8.3208e-04],\n",
      "        [-6.0074e-01,  1.1328e-01,  7.0018e-04],\n",
      "        [-6.0628e-01,  1.0867e-01,  5.2118e-04],\n",
      "        [-6.1227e-01,  9.6312e-02,  4.7737e-04],\n",
      "        [-6.1715e-01,  8.6425e-02,  6.5532e-04],\n",
      "        [-6.1940e-01,  8.8798e-02,  9.6831e-04],\n",
      "        [-6.1931e-01,  1.0650e-01,  1.2667e-03],\n",
      "        [-6.0992e-01,  1.2750e-01,  1.4426e-03],\n",
      "        [-5.9207e-01,  1.3070e-01,  1.4368e-03],\n",
      "        [-5.6442e-01,  9.0877e-02,  1.1816e-03],\n",
      "        [-5.2801e-01,  2.0817e-02,  8.8620e-04],\n",
      "        [-4.9609e-01, -4.5914e-02,  7.3436e-04],\n",
      "        [-4.9072e-01, -7.8075e-02,  8.2880e-04],\n",
      "        [-5.1423e-01, -7.0683e-02,  1.0861e-03],\n",
      "        [-5.6144e-01, -3.9872e-02,  1.2712e-03],\n",
      "        [-6.0589e-01, -6.8496e-03,  1.2955e-03],\n",
      "        [-6.1780e-01,  3.8478e-03,  1.2216e-03],\n",
      "        [-5.9327e-01, -1.2241e-02,  1.0904e-03],\n",
      "        [-5.5305e-01, -4.5051e-02,  9.4739e-04],\n",
      "        [-5.3079e-01, -7.3935e-02,  8.2380e-04],\n",
      "        [-5.4817e-01, -8.1810e-02,  8.0472e-04],\n",
      "        [-5.9370e-01, -6.7622e-02,  9.1869e-04],\n",
      "        [-6.3520e-01, -4.3140e-02,  1.1244e-03],\n",
      "        [-6.4330e-01, -3.9743e-03,  1.4343e-03],\n",
      "        [-6.0925e-01,  2.4735e-02,  1.6537e-03],\n",
      "        [-5.5640e-01,  2.9965e-02,  1.6958e-03],\n",
      "        [-5.1678e-01,  9.3200e-03,  1.5662e-03],\n",
      "        [-5.0180e-01, -3.3681e-02,  1.3803e-03],\n",
      "        [-5.0626e-01, -7.7283e-02,  1.2912e-03],\n",
      "        [-5.0760e-01, -1.0349e-01,  1.3318e-03],\n",
      "        [-4.9874e-01, -1.0640e-01,  1.5211e-03],\n",
      "        [-4.8079e-01, -8.8612e-02,  1.7675e-03],\n",
      "        [-4.5921e-01, -6.8775e-02,  1.8806e-03],\n",
      "        [-4.5878e-01, -6.2529e-02,  1.8041e-03],\n",
      "        [-4.8655e-01, -7.2502e-02,  1.5709e-03],\n",
      "        [-5.2691e-01, -9.3586e-02,  1.3213e-03],\n",
      "        [-5.6074e-01, -1.1503e-01,  1.1960e-03],\n",
      "        [-5.8569e-01, -1.3074e-01,  1.2372e-03],\n",
      "        [-6.0319e-01, -1.3597e-01,  1.4197e-03],\n",
      "        [-6.2649e-01, -1.3851e-01,  1.5955e-03],\n",
      "        [-6.4882e-01, -1.4430e-01,  1.5716e-03],\n",
      "        [-6.6067e-01, -1.6020e-01,  1.3788e-03],\n",
      "        [-6.5072e-01, -1.7148e-01,  1.2119e-03],\n",
      "        [-6.0256e-01, -1.6237e-01,  1.1288e-03],\n",
      "        [-5.4854e-01, -1.4190e-01,  1.1124e-03],\n",
      "        [-5.0622e-01, -1.2541e-01,  1.1365e-03],\n",
      "        [-4.8147e-01, -1.1685e-01,  1.1697e-03],\n",
      "        [-4.5053e-01, -1.1373e-01,  1.3183e-03],\n",
      "        [-4.2668e-01, -1.0991e-01,  1.4406e-03],\n",
      "        [-4.1372e-01, -1.0169e-01,  1.4947e-03],\n",
      "        [-4.1886e-01, -8.8675e-02,  1.4421e-03],\n",
      "        [-4.4229e-01, -7.3424e-02,  1.3147e-03],\n",
      "        [-4.5745e-01, -7.7404e-02,  1.0627e-03],\n",
      "        [-4.5890e-01, -9.9030e-02,  9.0528e-04],\n",
      "        [-4.5942e-01, -1.2922e-01,  9.3132e-04],\n",
      "        [-4.6014e-01, -1.5659e-01,  1.1066e-03],\n",
      "        [-4.7108e-01, -1.6262e-01,  1.3319e-03],\n",
      "        [-4.7869e-01, -1.4987e-01,  1.6021e-03],\n",
      "        [-4.9747e-01, -1.3735e-01,  1.6256e-03],\n",
      "        [-5.1628e-01, -1.4004e-01,  1.3791e-03],\n",
      "        [-5.2310e-01, -1.5939e-01,  1.0175e-03],\n",
      "        [-5.1333e-01, -1.8738e-01,  7.7611e-04],\n",
      "        [-5.0245e-01, -2.0501e-01,  7.4223e-04],\n",
      "        [-5.0308e-01, -1.9468e-01,  8.8108e-04],\n",
      "        [-5.1823e-01, -1.6484e-01,  1.1499e-03],\n",
      "        [-5.2770e-01, -1.3082e-01,  1.4118e-03],\n",
      "        [-5.1329e-01, -1.0512e-01,  1.5506e-03],\n",
      "        [-4.6814e-01, -9.6464e-02,  1.6146e-03],\n",
      "        [-4.0690e-01, -1.0395e-01,  1.6071e-03],\n",
      "        [-3.6262e-01, -1.2167e-01,  1.5267e-03],\n",
      "        [-3.6441e-01, -1.4011e-01,  1.4465e-03],\n",
      "        [-4.0829e-01, -1.4366e-01,  1.4392e-03],\n",
      "        [-4.6271e-01, -1.2923e-01,  1.5124e-03],\n",
      "        [-4.9518e-01, -1.0032e-01,  1.6896e-03],\n",
      "        [-4.9367e-01, -6.6021e-02,  1.9085e-03],\n",
      "        [-4.6642e-01, -4.1583e-02,  1.9760e-03],\n",
      "        [-4.4653e-01, -4.0616e-02,  1.8554e-03],\n",
      "        [-4.5709e-01, -6.5868e-02,  1.6338e-03],\n",
      "        [-4.9727e-01, -9.6936e-02,  1.4980e-03],\n",
      "        [-5.5075e-01, -1.1332e-01,  1.4992e-03],\n",
      "        [-5.9305e-01, -1.0853e-01,  1.6823e-03],\n",
      "        [-6.1468e-01, -8.4980e-02,  1.9358e-03],\n",
      "        [-6.1047e-01, -5.6979e-02,  2.1286e-03],\n",
      "        [-5.9378e-01, -4.8744e-02,  2.1023e-03],\n",
      "        [-5.6971e-01, -7.5572e-02,  1.8729e-03],\n",
      "        [-5.3861e-01, -1.3224e-01,  1.5472e-03],\n",
      "        [-5.0503e-01, -1.9062e-01,  1.3430e-03],\n",
      "        [-4.7126e-01, -2.2260e-01,  1.3663e-03],\n",
      "        [-4.3861e-01, -2.1463e-01,  1.5587e-03],\n",
      "        [-4.1524e-01, -1.7991e-01,  1.7768e-03],\n",
      "        [-4.0259e-01, -1.4087e-01,  1.9085e-03],\n",
      "        [-3.9608e-01, -1.0961e-01,  1.8930e-03],\n",
      "        [-3.8838e-01, -8.8097e-02,  1.7772e-03],\n",
      "        [-3.8254e-01, -7.6098e-02,  1.6470e-03],\n",
      "        [-3.8377e-01, -6.9275e-02,  1.5331e-03],\n",
      "        [-4.0217e-01, -7.4331e-02,  1.4261e-03]])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"display:inline-block; width: 150px; background-color: white; margin-right:10px;\"><svg viewBox=\"0 0 128 128\"><g stroke-width=\"0.8\">\n",
       "<path d=\"M 64.0 36.0 L 56.0 20.0 L 49.0 13.0 L 54.0 39.0 L 38.0 41.0 L 22.86018943786621 37.44416403770447 L 9.134223937988281 29.918788194656372 L -2.1517200469970703 19.88708806037903 L -11.002891540527344 8.125345468521118 L -17.7383975982666 -4.86513876914978 L -22.526321411132812 -19.42219042778015 L -25.374666213989258 -35.92465281486511 L -25.916790008544922 -54.45372653007507 L -23.3610782623291 -74.69116282463074 L -17.641162872314453 -93.11485362052917 L -11.640012741088867 -106.96593260765076 L -5.941965103149414 -115.02799487113953 L -1.582601547241211 -117.57975673675537 L 0.4557609558105469 -117.0274937748909 L 0.9922466278076172 -115.7415423989296 L 1.108755111694336 -114.28527623414993 L 1.0539913177490234 -112.99795347452164 L 0.7662601470947266 -112.14200437068939 L 0.3558216094970703 -111.3602768778801 L -0.16173171997070312 -110.61588859558105 L -0.7345867156982422 -109.8984768986702 L -1.2987918853759766 -109.18850100040436 L -1.8239307403564453 -108.48439019918442 L -2.328350067138672 -107.78298115730286 L -2.8550891876220703 -107.08569294214249 L -3.425914764404297 -106.40449893474579 L -4.02039909362793 -105.7459813952446 L -4.589237213134766 -105.10759782791138 L -5.087881088256836 -104.46991330385208 L -5.542333602905273 -103.80773651599884 L -5.971410751342773 -103.10875076055527 L -6.401834487915039 -102.38512969017029 L -6.866722106933594 -101.67104059457779 L -7.383201599121094 -100.99832952022552 L -7.942501068115234 -100.37520569562912 L -8.522455215454102 -99.78274726867676 L -9.117900848388672 -99.19775420427322 L -9.737823486328125 -98.60405790805817 L -10.383817672729492 -98.00341695547104 L -11.043645858764648 -97.41222500801086 L -11.69416618347168 -96.84980541467667 L -12.307811737060547 -96.33058726787567 L -12.888137817382812 -95.85531920194626 L -13.478254318237305 -95.41979837417603 L -14.084728240966797 -95.02018147706985 L -14.69282341003418 -94.65380394458771 L -15.283758163452148 -94.31412452459335 L -15.867866516113281 -93.98898768424988 L -16.429494857788086 -93.66878801584244 L -16.95454978942871 -93.350146651268 L -17.44756317138672 -93.04640263319016 L -17.919218063354492 -92.77579498291016 L -18.390939712524414 -92.54376345872879 L -18.88142204284668 -92.33102095127106 L -19.392547607421875 -92.10010713338852 L -19.92775535583496 -91.81636929512024 L -20.50139808654785 -91.47481113672256 L -21.1171817779541 -91.09947860240936 L -21.761371612548828 -90.73042410612106 L -22.40432357788086 -90.39502573013306 L -23.045421600341797 -90.09524136781693 L -23.711076736450195 -89.8132392168045 L -24.42033576965332 -89.52360481023788 L -25.17059326171875 -89.21726393699646 L -25.939746856689453 -88.90183073282242 L -26.688440322875977 -88.59342133998871 L -27.37918472290039 -88.30799835920334 L -28.011837005615234 -88.05573844909668 L -28.615917205810547 -87.83298045396805 L -29.226781845092773 -87.63176119327545 L -29.861713409423828 -87.43461030721664 L -30.507221221923828 -87.22344517707825 L -31.13906478881836 -86.9896712899208 L -31.744298934936523 -86.73800361156464 L -32.333566665649414 -86.47048395872116 L -32.93539047241211 -86.19966840744019 L -33.5642032623291 -85.94239503145218 L -34.21735763549805 -85.70676720142365 L -34.881608963012695 -85.49254459142685 L -35.53547286987305 -85.29331564903259 L -36.17546081542969 -85.1008625626564 L -36.812971115112305 -84.91257703304291 L -37.46314239501953 -84.73568791151047 L -38.12726020812988 -84.58271503448486 L -38.79474639892578 -84.4574254155159 L -39.435930252075195 -84.34082472324371 L -40.03267288208008 -84.21132844686508 L -40.58415222167969 -84.05834603309631 L -41.10477256774902 -83.88403660058975 L -41.619651794433594 -83.69700801372528 L -42.14573287963867 -83.51645869016647 L -42.67262649536133 -83.34775018692017 L -43.19172668457031 -83.19218236207962 L -43.71406173706055 -83.05174362659454 L -44.26007652282715 -82.93162769079208 L -44.829816818237305 -82.83506464958191 L -45.418745040893555 -82.75731760263443 L -46.01981735229492 -82.68611180782318 L -46.62519454956055 -82.60397881269455 L -47.229143142700195 -82.50222587585449 L -47.82988739013672 -82.3889496922493 L -48.4361629486084 -82.28028357028961 L -49.04843521118164 -82.18397134542465 L -49.66558074951172 -82.09754681587219 L -50.28498458862305 -82.0087485909462 L -50.90429496765137 -81.9022444486618 L -51.51421356201172 -81.77474516630173 L -52.10628700256348 -81.64404630661011 L -52.670705795288086 -81.55316907167435 L -53.19871520996094 -81.53235161304474 L -53.694801330566406 -81.57826560735703 L -54.18552589416504 -81.65634036064148 L -54.69975662231445 -81.72702318429947 L -55.261199951171875 -81.76689565181732 L -55.86709022521973 -81.77374523878098 L -56.48489189147949 -81.7698974609375 L -57.07816123962402 -81.7821381688118 L -57.63121032714844 -81.82718908786774 L -58.16200256347656 -81.90112394094467 L -58.71017646789551 -81.98293423652649 L -59.30387306213379 -82.05055671930313 L -59.93907165527344 -82.09369647502899 L -60.58237648010254 -82.09767073392868 L -61.19162178039551 -82.07293558120728 L -61.748023986816406 -82.0429704785347 L -62.264801025390625 -82.03365051746368 L -62.766605377197266 -82.06733173131943 L -63.27286720275879 -82.14461493492126 L -63.780466079711914 -82.24810606241226 L -64.27920913696289 -82.35450112819672 L -64.7599983215332 -82.44311302900314 L -65.21920585632324 -82.511887550354 L -65.67798805236816 -82.57441645860672 L -66.16453742980957 -82.64691889286041 L -66.69145202636719 -82.74050515890121 L -67.25219345092773 -82.85553288459778 L -67.8378791809082 -82.98626953363419 L -68.44107055664062 -83.12223517894745 L -69.06756401062012 -83.26074904203415 L -69.71638107299805 -83.40505266189575 L -70.3770523071289 -83.5652568936348 L -71.02776908874512 -83.73673450946808 L -71.63032531738281 -83.89910739660263 L -72.17886352539062 -84.04100489616394 L -72.68508720397949 -84.16641908884048 L -73.16656112670898 -84.28326642513275 L -73.61709403991699 -84.39699238538742 L -74.04377746582031 -84.50689888000488 L -74.45749473571777 -84.6085894703865 L -74.87635231018066 -84.69726431369781 L -75.31863975524902 -84.77068799734116 L -75.77608871459961 -84.84809231758118 L -76.2349910736084 -84.94712215662003 L -76.69441223144531 -85.07634246349335 L -77.1545524597168 -85.23293608427048 L -77.6256275177002 -85.39555406570435 L -78.10432052612305 -85.54542523622513 L -78.60178565979004 -85.68277180194855 L -79.11806869506836 -85.82281345129013 L -79.64117240905762 -85.98220705986023 L -80.15449905395508 -86.169582426548 L -80.65694999694824 -86.37459313869476 L -81.1600284576416 -86.56926888227463 L -81.67825508117676 -86.73411273956299 L -82.2059555053711 -86.86492854356766 L -82.71924591064453 -86.97005236148834 L -83.18738555908203 -87.06651681661606 L -83.59428215026855 -87.17046189308167 L -83.95690155029297 -87.292131960392 L -84.32131576538086 -87.43224322795868 L -84.72960472106934 -87.57590788602829 L -85.19231414794922 -87.70513868331909 L -85.68749046325684 -87.80545979738235 L -86.18115615844727 -87.87148106098175 L -86.6475715637207 -87.91306442022324 L -87.0941047668457 -87.95368075370789 L -87.55119132995605 -88.01954847574234 L -88.0484619140625 -88.11648404598236 L -88.59921264648438 -88.22980850934982 L -89.19225883483887 -88.33833885192871 L -89.80694198608398 -88.42331916093826 L -90.41740798950195 -88.4802976846695 L -91.01118469238281 -88.52904218435287 L -91.58089447021484 -88.60461449623108 L -92.1195068359375 -88.73685032129288 L -92.62453651428223 -88.9274662733078 L -93.09579277038574 -89.1500636935234 L -93.53440475463867 -89.3646912574768 L -93.94964790344238 -89.54460602998734 L -94.35223770141602 -89.68547451496124 L -94.7483139038086 -89.79508823156357 L -95.13669395446777 -89.88318514823914 L -95.51923179626465 -89.95928293466568 L -95.90300178527832 -90.02855813503265 L -96.30517196655273 -90.1028887629509\" stroke=\"black\" fill=\"none\"/>\n",
       "</g></svg></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate(model, start_seq, max_len=200):\n",
    "    \"\"\"\n",
    "    Autoregressively generate strokes from a start sequence.\n",
    "    start_seq: (1, start_len, 3)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    start_seq = start_seq.to(device)\n",
    "    generated = start_seq.clone()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Encode once\n",
    "        enc_output = model.encoder(start_seq)\n",
    "\n",
    "        for _ in range(max_len - start_seq.size(1)):\n",
    "            # Decoder input: generated sequence so far\n",
    "            dec_input = generated\n",
    "\n",
    "            dec_output = model.decoder(dec_input, enc_output)  # (1, seq_len, d_model)\n",
    "            next_step = model.output_layer(\n",
    "                dec_output[:, -1:, :]\n",
    "            )  # last step prediction\n",
    "\n",
    "            # Append predicted step\n",
    "            generated = torch.cat([generated, next_step], dim=1)\n",
    "\n",
    "    return generated\n",
    "\n",
    "\n",
    "# Example usage\n",
    "start_seq = training_data[0]  # (seq_len, 3)\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "original_svg = tensor_to_svg_strokes(start_seq, size=128)\n",
    "display(\n",
    "    HTML(\n",
    "        f'<div style=\"display:inline-block; width: 150px; background-color: white; margin-right:10px;\">{original_svg}</div>'\n",
    "    )\n",
    ")\n",
    "\n",
    "# remove some of the end to simulate partial input\n",
    "start_seq = start_seq[:5, :]  # (start_len, 3)\n",
    "start_seq = normalize_strokes(start_seq.unsqueeze(0))  # (1, seq_len, 3)\n",
    "\n",
    "generated_seq = generate(model, start_seq, max_len=200)\n",
    "print(\"Generated shape:\", generated_seq.shape)\n",
    "\n",
    "# denormalize\n",
    "generated_seq = denormalize_strokes(generated_seq.squeeze(0)).cpu()\n",
    "print(generated_seq)\n",
    "\n",
    "\n",
    "svg = tensor_to_svg_strokes(generated_seq, size=128)\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        f'<div style=\"display:inline-block; width: 150px; background-color: white; margin-right:10px;\">{svg}</div>'\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
